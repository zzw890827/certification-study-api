[
  {
    "text": "A company makes forecasts each quarter to decide how to optimize operations to meet expected demand. The company uses ML models to make these forecasts. An AI practitioner is writing a report about the trained ML models to provide transparency and explainability to company stakeholders. What should the AI practitioner include in the report to meet the transparency and explainability requirements?",
    "explanations": [
      "While useful for reproducibility and auditing, raw code does not directly help non-technical stakeholders understand model behavior.",
      "Partial dependence plots (PDPs) are a popular tool for model explainability. They show the relationship between a subset of features (typically one or two) and the predicted outcome, averaging out the effects of all other features. This helps non-technical stakeholders understand how a model behaves.",
      "This is useful for context but doesn’t directly help explain model decisions. It can also raise privacy or compliance concerns.",
      "SThese show if the model training has mathematically converged, which is important for model quality, but not for interpretability."
    ],
    "type": "single",
    "options": [
      "Code for model training",
      "Partial dependence plots (PDPs)",
      "Sample data for training",
      "Model convergence tables"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A law firm wants to build an AI application by using large language models (LLMs). The application will read legal documents and extract key points from the documents. Which solution meets these requirements?",
    "explanations": [
      "This identifies entities like names, dates, or organizations, but it does not extract or summarize key points in a document.",
      "Recommendation systems suggest items or content — not suited for document summarization.",
      "The goal is to read legal documents and extract key points — this is fundamentally a summarization task, and LLMs are well-suited for it. Understand and process large legal documents. Extract and present key information or summaries in a human-friendly format. Potentially allow users to ask follow-up questions about the summarized content. This meets the requirement of extracting key points in a way that supports user interaction and clarity.",
      "Recommendation systems suggest items or content — not suited for document summarization."
    ],
    "type": "single",
    "options": [
      "Build an automatic named entity recognition system.",
      "Create a recommendation engine.",
      "Develop a summarization chatbot.",
      "Develop a multi-language translation system."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to classify human genes into 20 categories based on gene characteristics. The company needs an ML algorithm to document how the inner mechanism of the model affects the output. Which ML algorithm meets these requirements?",
    "explanations": [
      "Classification of genes into 20 categories → This is a multi-class classification problem. Transparency into how the model makes decisions → The model must be interpretable, meaning its inner mechanisms (feature impact and decision logic) can be documented and understood. They provide a clear, hierarchical structure showing how decisions are made. Each split in the tree corresponds to a specific rule based on input features. You can trace exactly how a prediction is made, step by step. Trees are naturally interpretable and often used when transparency is a requirement.",
      "Not suitable — it’s for regression (predicting continuous values), not classification.",
      "It’s suitable for classification and interpretable, but: 1. Standard logistic regression handles binary classification. 2. Multinomial logistic regression can be used for 20 classes, but it’s less intuitive than decision trees when documenting decision logic.",
      "Not suitable — they are black-box models. While powerful, they require tools like SHAP or LIME for post-hoc explanation, which doesn’t provide native interpretability."
    ],
    "type": "single",
    "options": [
      "Decision trees",
      "Linear regression",
      "Logistic regression",
      "Neural networks"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has built an image classification model to predict plant diseases from photos of plant leaves. The company wants to evaluate how many images the model classified correctly. Which evaluation metric should the company use to measure the model's performance?",
    "explanations": [
      "Used for regression problems, not classification.",
      "The company is working on an image classification problem — predicting discrete labels (types of plant diseases) from images. Since the goal is to evaluate how many images were correctly classified, the most straightforward and appropriate evaluation metric is: Accuracy = (Number of correct predictions) / (Total number of predictions)",
      "Also for regression tasks — measures error between predicted and actual continuous values.",
      "This is a hyperparameter used during training, not an evaluation metric."
    ],
    "type": "single",
    "options": [
      "R-squared score",
      "Accuracy",
      "Root mean squared error (RMSE)",
      "Learning rate"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is using a pre-trained large language model (LLM) to build a chatbot for product recommendations. The company needs the LLM outputs to be short and written in a specific language. Which solution will align the LLM response quality with the company's expectations?",
    "explanations": [
      "To control the length and language of the LLM's outputs, prompt engineering is the most direct and effective approach. By clearly specifying in the prompt what the chatbot should do (e.g., Respond in short sentences in Japanese), the LLM can be guided to produce responses that align with the company’s expectations.",
      "Choose an LLM of a different size: May impact performance or capability, but does not directly address output length or language.",
      "Increase the temperature: Affects randomness/creativity, not control over length or language.",
      "Increase the Top K value: Also affects sampling diversity, not output structure or language control."
    ],
    "type": "single",
    "options": [
      "Adjust the prompt.",
      "Choose an LLM of a different size.",
      "Increase the temperature.",
      "Increase the Top K value."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company uses Amazon SageMaker for its ML pipeline in a production environment. The company has large input data sizes up to 1 GB and processing times up to 1 hour. The company needs near real-time latency. Which SageMaker inference option meets these requirements?",
    "explanations": [
      "Real-time inference: Designed for low-latency, short-duration requests. Not suitable for large data or long processing times.",
      "Serverless inference: Good for sporadic, low-latency workloads, but it has payload size and timeout limits that don't suit 1 GB/1-hour jobs.",
      "The company's scenario involves: Large input data sizes (up to 1 GB); Long processing times (up to 1 hour); Need for near real-time latency (not immediate, but faster than batch). Asynchronous inference is specifically designed for: Handling large payloads (up to several GBs); Supporting long-running inference requests (up to 1 hour); Providing near real-time responses through callbacks or polling",
      "Batch transform: Suitable for offline batch jobs, not near real-time inference."
    ],
    "type": "single",
    "options": [
      "Real-time inference",
      "Serverless inference",
      "Asynchronous inference",
      "Batch transform"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is using domain-specific models. The company wants to avoid creating new models from the beginning. The company instead wants to adapt pre-trained models to create models for new, related tasks. Which ML strategy meets these requirements?",
    "explanations": [
      "Increase the number of epochs: A training parameter, not a strategy for adapting existing models.",
      "Transfer learning allows a company to: Leverage pre-trained models (often trained on large, general datasets), And adapt them to new, related tasks using smaller, domain-specific datasets. This avoids the need to train a model from scratch, saving both time and computational resources while still achieving good performance on the new task.",
      "Decrease the number of epochs: Also just a tuning parameter, not relevant to reusing models.",
      "Use unsupervised learning: Doesn’t fit the goal of adapting pre-trained models to related supervised tasks."
    ],
    "type": "single",
    "options": [
      "Increase the number of epochs.",
      "Use transfer learning.",
      "Decrease the number of epochs.",
      "Use unsupervised learning."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is building a solution to generate images for protective eyewear. The solution must have high accuracy and must minimize the risk of incorrect annotations. Which solution will meet these requirements?",
    "explanations": [
      "When high accuracy and minimizing incorrect annotations are essential—especially for image data used in model training—the most reliable approach is to incorporate human-in-the-loop (HITL) validation. Amazon SageMaker Ground Truth Plus:Provides high-quality labeled data by integrating human review where needed. Minimizes annotation errors by combining machine learning with professional human labelers. Is ideal for critical applications like protective equipment where safety and precision are key.",
      "Data augmentation by using Amazon Bedrock knowledge base: Bedrock is for LLMs, not image data or labeling.",
      "Image recognition by using Amazon Rekognition: Good for predefined object detection, but not suited for custom, image generation or labeling.",
      "Data summarization by using Amazon QuickSight Q: Used for business intelligence, not image annotation or model training."
    ],
    "type": "single",
    "options": [
      "Human-in-the-loop validation by using Amazon SageMaker Ground Truth Plus",
      "Data augmentation by using an Amazon Bedrock knowledge base",
      "Image recognition by using Amazon Rekognition",
      "Data summarization by using Amazon QuickSight Q"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to create a chatbot by using a foundation model (FM) on Amazon Bedrock. The FM needs to access encrypted data that is stored in an Amazon S3 bucket. The data is encrypted with Amazon S3 managed keys (SSE-S3). The FM encounters a failure when attempting to access the S3 bucket data. Which solution will meet these requirements?",
    "explanations": [
      "Amazon Bedrock requires proper IAM permissions to access and decrypt S3 data. Even though the data is encrypted with Amazon S3 managed keys (SSE-S3), Bedrock must assume a role that has the necessary permissions (such as 's3:GetObject') for the bucket and object. Since SSE-S3 does not require explicit key permissions (as AWS manages the keys), ensuring that the role has S3 access is essential to resolve the failure.",
      "Setting S3 buckets to allow public access is a serious security risk and goes against best practices, especially for encrypted and sensitive data. Public access is not required for Bedrock or other AWS services that use IAM roles for secure access.",
      "Prompt engineering influences the behavior of the foundation model, not its access to external data. It cannot grant access or resolve permission issues related to S3 buckets.",
      "Ensuring data is not sensitive is a general data management concern, but it does not address the access failure. The failure is due to permission issues, not data sensitivity."
    ],
    "type": "single",
    "options": [
      "Ensure that the role that Amazon Bedrock assumes has permission to decrypt data with the correct encryption key.",
      "Set the access permissions for the S3 buckets to allow public access to enable access over the internet.",
      "Use prompt engineering techniques to tell the model to look for information in Amazon S3.",
      "Ensure that the S3 data does not contain sensitive information."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to use language models to create an application for inference on edge devices. The inference must have the lowest latency possible. Which solution will meet these requirements?",
    "explanations": [
      "Deploying optimized small language models (SLMs) directly on edge devices ensures that inference happens locally, eliminating the need for network communication and thus achieving the **lowest possible latency**. SLMs are lightweight enough to run efficiently on edge hardware.",
      "Large language models (LLMs) require significant computational resources and are not typically suitable for deployment on edge devices due to memory and processing constraints, resulting in poor performance and higher latency.",
      "Using a centralized SLM API introduces network latency due to remote communication, which contradicts the requirement for the **lowest possible latency**.",
      "This approach combines both drawbacks — centralized processing and large model size — leading to **higher latency** and resource limitations, making it the worst option for edge deployment."
    ],
    "type": "single",
    "options": [
      "Deploy optimized small language models (SLMs) on edge devices.",
      "Deploy optimized large language models (LLMs) on edge devices.",
      "Incorporate a centralized small language model (SLM) API for asynchronous communication with edge devices.",
      "Incorporate a centralized large language model (LLM) API for asynchronous communication with edge devices."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to build an ML model by using Amazon SageMaker. The company needs to share and manage variables for model development across multiple teams. Which SageMaker feature meets these requirements?",
    "explanations": [
      "Amazon SageMaker Feature Store is specifically designed to store, share, and manage features (i.e., variables) for machine learning across different teams and models. It supports feature consistency, reuse, and governance, making it ideal for collaborative model development.",
      "SageMaker Data Wrangler is used for data preparation and transformation, not for sharing and managing features across teams.",
      "SageMaker Clarify is used to detect bias and explain model predictions, not for managing shared variables or features.",
      "SageMaker Model Cards are used for documenting and sharing model metadata, not for sharing or managing features or variables used during development."
    ],
    "type": "single",
    "options": [
      "Amazon SageMaker Feature Store",
      "Amazon SageMaker Data Wrangler",
      "Amazon SageMaker Clarify",
      "Amazon SageMaker Model Cards"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to use generative AI to increase developer productivity and software development. The company wants to use Amazon Q Developer. What can Amazon Q Developer do to help the company meet these requirements?",
    "explanations": [
      "Amazon Q Developer is designed to assist software developers by generating code snippets, providing reference tracking, and helping with open source license tracking. These capabilities directly support increased productivity in software development workflows.",
      "Running applications without managing servers describes AWS Lambda or similar serverless services, not Amazon Q Developer.",
      "While Amazon Q can process natural language queries, enabling voice commands is not a primary feature of Amazon Q Developer.",
      "Converting audio files to text is a transcription task typically handled by Amazon Transcribe, not Amazon Q Developer."
    ],
    "type": "single",
    "options": [
      "Create software snippets, reference tracking, and open source license tracking.",
      "Run an application without provisioning or managing servers.",
      "Enable voice commands for coding and providing natural language search.",
      "Convert audio files to text documents by using ML models."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A financial institution is using Amazon Bedrock to develop an AI application. The application is hosted in a VPC. To meet regulatory compliance standards, the VPC is not allowed access to any internet traffic. Which AWS service or feature will meet these requirements?",
    "explanations": [
      "AWS PrivateLink enables **private connectivity** between VPCs and AWS services without using the public internet. It is ideal for scenarios where compliance requires **no internet traffic**, such as for financial institutions using Amazon Bedrock in a VPC.",
      "Amazon Macie is a data security service used to detect sensitive data in S3, and it does not provide private connectivity or network isolation.",
      "WAmazon CloudFront is a content delivery network (CDN) that uses the public internet and is not suitable for VPCs with strict no-internet policies.",
      "An internet gateway explicitly allows internet access from a VPC, which violates the regulatory requirement to restrict all internet traffic."
    ],
    "type": "single",
    "options": [
      "AWS PrivateLink",
      "Amazon Macie",
      "Amazon CloudFront",
      "Internet gateway"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to develop an educational game where users answer questions such as the following: A jar contains six red, four green, and three yellow marbles. What is the probability of choosing a green marble from the jar? Which solution meets these requirements with the LEAST operational overhead?",
    "explanations": [
      "The question involves basic probability that can be accurately and efficiently solved using **simple rule-based logic or deterministic computations**. Implementing this with code (e.g., probability = number of favorable outcomes / total outcomes) involves the **least operational overhead** and is the most straightforward solution.",
      "Supervised learning and regression are used for learning from labeled data and predicting continuous outcomes, but this problem does not require model training or data patterns—it is a direct calculation.",
      "Reinforcement learning is used for decision-making tasks involving rewards and actions over time. It is highly complex and inappropriate for a simple probability question.",
      "Unsupervised learning and probability density estimation are used to discover patterns in data without labels. This is unnecessary and overengineered for computing straightforward mathematical probabilities."
    ],
    "type": "single",
    "options": [
      "Use supervised learning to create a regression model that will predict probability.",
      "Use reinforcement learning to train a model to return the probability.",
      "Use code that will calculate probability by using simple rules and computations.",
      "Use unsupervised learning to create a model that will estimate probability density."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "Which metric measures the runtime efficiency of operating AI models?",
    "explanations": [
      "Average response time reflects how quickly an AI model returns results during inference, making it a direct measure of runtime efficiency.",
      "Customer satisfaction score (CSAT) measures user sentiment and experience, not technical performance.",
      "Training time for each epoch relates to training phase efficiency, not model runtime.",
      "Number of training instances indicates dataset size, not how efficiently the model performs during operation."
    ],
    "type": "single",
    "options": [
      "Customer satisfaction score (CSAT)",
      "Training time for each epoch",
      "Average response time",
      "Number of training instances"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is building a contact center application and wants to gain insights from customer conversations. The company wants to analyze and extract key information from the audio of the customer calls. Which solution meets these requirements?",
    "explanations": [
      "Amazon Transcribe is the appropriate service to convert speech from audio recordings into text, enabling further analysis and extraction of key information from customer conversations.",
      "Amazon Lex is used to build conversational interfaces like chatbots, not for analyzing recorded audio.",
      "Amazon SageMaker Model Monitor is used for detecting data drift and model quality issues in deployed ML models, not for processing or extracting insights from audio recordings.",
      "Amazon Comprehend can analyze text for sentiment and key phrases, but it requires transcribed text input, which makes Amazon Transcribe the necessary first step."
    ],
    "type": "single",
    "options": [
      "Build a conversational chatbot by using Amazon Lex.",
      "Transcribe call recordings by using Amazon Transcribe.",
      "Extract information from call recordings by using Amazon SageMaker Model Monitor.",
      "Create classification labels by using Amazon Comprehend."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company has petabytes of unlabeled customer data to use for an advertisement campaign. The company wants to classify its customers into tiers to advertise and promote the company's products. Which methodology should the company use to meet these requirements?",
    "explanations": [
      "Unsupervised learning is ideal for analyzing large amounts of unlabeled data to discover hidden patterns or groupings, such as customer segmentation into tiers.",
      "Supervised learning requires labeled data, which is not available in this case.",
      "Reinforcement learning is used for decision-making tasks with defined rewards, not for clustering unlabeled data.",
      "RLHF is used to fine-tune models based on human feedback, typically in natural language tasks, and is not appropriate for customer classification from raw, unlabeled data."
    ],
    "type": "single",
    "options": [
      "Supervised learning",
      "Unsupervised learning",
      "Reinforcement learning",
      "Reinforcement learning from human feedback (RLHF)"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An AI practitioner wants to use a foundation model (FM) to design a search application. The search application must handle queries that have text and images. Which type of FM should the AI practitioner use to power the search application?",
    "explanations": [
      "A multi-modal embedding model can process and encode both text and images into a shared vector space, enabling the search application to understand and match multi-modal queries effectively.",
      "A text embedding model only supports textual input and cannot process image queries.",
      "A multi-modal generation model is designed to generate content from multi-modal inputs, not to support search or retrieval tasks.",
      "An image generation model creates images from text or other prompts but does not provide the embedding functionality needed for search applications."
    ],
    "type": "single",
    "options": [
      "Multi-modal embedding model",
      "Text embedding model",
      "Multi-modal generation model",
      "Image generation model"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company uses a foundation model (FM) from Amazon Bedrock for an AI search tool. The company wants to fine-tune the model to be more accurate by using the company's data. Which strategy will successfully fine-tune the model?",
    "explanations": [
      "Fine-tuning a foundation model typically requires structured, labeled training data that includes both a prompt (input) and a completion (expected output). This allows the model to learn how to respond more accurately to similar inputs.",
      "Creating a .txt file in .csv format is a formatting step, not a valid fine-tuning strategy by itself.",
      "Provisioned Throughput in Amazon Bedrock improves performance and scalability but does not relate to fine-tuning the model.",
      "Training the model on journals and textbooks refers to pretraining from scratch, not fine-tuning, and is not supported directly through Bedrock."
    ],
    "type": "single",
    "options": [
      "Provide labeled data with the prompt field and the completion field.",
      "Prepare the training dataset by creating a .txt file that contains multiple lines in .csv format.",
      "Purchase Provisioned Throughput for Amazon Bedrock.",
      "Train the model on journals and textbooks."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to use AI to protect its application from threats. The AI solution needs to check if an IP address is from a suspicious source. Which solution meets these requirements?",
    "explanations": [
      "An anomaly detection system is suitable for identifying unusual patterns or outliers, such as suspicious IP addresses, which may indicate security threats.",
      "A speech recognition system processes audio to text and is irrelevant for detecting IP-based threats.",
      "A named entity recognition system identifies entities like names or organizations in text, not suspicious IP addresses.",
      "A fraud forecasting system may focus on predicting financial fraud but is not specific to identifying anomalous IP behavior in real-time threat protection contexts."
    ],
    "type": "single",
    "options": [
      "Build a speech recognition system.",
      "Create a natural language processing (NLP) named entity recognition system.",
      "Develop an anomaly detection system.",
      "Create a fraud forecasting system."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "Which feature of Amazon OpenSearch Service gives companies the ability to build vector database applications?",
    "explanations": [
      "Scalable index management and nearest neighbor search capability enables vector search in Amazon OpenSearch Service, which is essential for building vector database applications such as semantic search or similarity search.",
      "Integration with Amazon S3 is used for storing snapshots or logs, not for supporting vector search.",
      "Geospatial indexing and queries are focused on location-based data, not vector embeddings.",
      "Real-time analysis on streaming data is useful for analytics but does not support vector search or similarity matching needed for vector databases."
    ],
    "type": "single",
    "options": [
      "Integration with Amazon S3 for object storage",
      "Support for geospatial indexing and queries",
      "Scalable index management and nearest neighbor search capability",
      "Ability to perform real-time analysis on streaming data"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "Which option is a use case for generative AI models?",
    "explanations": [
      "Creating photorealistic images from text descriptions is a direct application of generative AI, which can generate new content such as images, text, or audio based on input prompts.",
      "Improving network security involves anomaly detection or pattern recognition, typically handled by traditional or discriminative AI models, not generative ones.",
      "Optimizing database indexing is a systems engineering task, not a generative AI use case.",
      "Forecasting stock trends involves time series analysis and predictive modeling, not generating new content."
    ],
    "type": "single",
    "options": [
      "Improving network security by using intrusion detection systems",
      "Creating photorealistic images from text descriptions for digital marketing",
      "Enhancing database performance by using optimized indexing",
      "Analyzing financial data to forecast stock market trends"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to build a generative AI application by using Amazon Bedrock and needs to choose a foundation model (FM). The company wants to know how much information can fit into one prompt. Which consideration will inform the company's decision?",
    "explanations": [
      "Context window refers to the maximum number of tokens (words or characters) a foundation model can process in a single prompt. It directly determines how much information can be included in a prompt.",
      "Temperature controls the randomness of the model's output, not the input size.",
      "Batch size relates to how many prompts can be processed simultaneously, not the size of each individual prompt.",
      "Model size refers to the number of parameters in the model, which affects performance and capabilities but not the length of a single prompt."
    ],
    "type": "single",
    "options": [
      "Temperature",
      "Context window",
      "Batch size",
      "Model size"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to make a chatbot to help customers. The chatbot will help solve technical problems without human intervention. The company chose a foundation model (FM) for the chatbot. The chatbot needs to produce responses that adhere to company tone. Which solution meets these requirements?",
    "explanations": [
      "Experimenting and refining the prompt is a core technique in prompt engineering. It helps guide the foundation model to produce responses that align with the desired tone, style, and content, such as adhering to a company’s communication standards.",
      "Setting a low token limit restricts response length but does not influence tone.",
      "Batch inferencing improves throughput by processing multiple prompts simultaneously, but it does not affect the style or tone of responses.",
      "Increasing the temperature leads to more randomness in output, which may result in inconsistent tone rather than controlled, company-specific language."
    ],
    "type": "single",
    "options": [
      "Set a low limit on the number of tokens the FM can produce.",
      "Use batch inferencing to process detailed responses.",
      "Experiment and refine the prompt until the FM produces the desired responses.",
      "Define a higher number for the temperature parameter."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company wants to classify the sentiment of text passages as positive or negative. Which prompt engineering strategy meets these requirements?",
    "explanations": [
      "Providing labeled examples in the prompt followed by a new input is known as few-shot prompting. It helps the LLM understand the task and perform accurate sentiment classification based on the given examples.",
      "Explaining how sentiment analysis and LLMs work adds unnecessary information and does not help the model perform the task better.",
      "Providing only the new text without context or examples is a zero-shot approach and may reduce classification accuracy compared to few-shot prompting.",
      "Including unrelated task examples like summarization or question answering confuses the model and reduces the effectiveness of the prompt for sentiment analysis."
    ],
    "type": "single",
    "options": [
      "Provide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classified.",
      "Provide a detailed explanation of sentiment analysis and how LLMs work in the prompt.",
      "Provide the new text passage to be classified without any additional context or examples.",
      "Provide the new text passage with a few examples of unrelated tasks, such as text summarization or question answering."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A security company is using Amazon Bedrock to run foundation models (FMs). The company wants to ensure that only authorized users invoke the models. The company needs to identify any unauthorized access attempts to set appropriate AWS Identity and Access Management (IAM) policies and roles for future iterations of the FMs. Which AWS service should the company use to identify unauthorized users that are trying to access Amazon Bedrock?",
    "explanations": [
      "AWS CloudTrail records all API calls made to AWS services, including Amazon Bedrock. It helps identify who accessed what service and whether the access was authorized or denied, making it ideal for auditing and security investigations.",
      "AWS Audit Manager helps with compliance audits but does not provide detailed logs of unauthorized access attempts.",
      "Amazon Fraud Detector is designed for detecting online fraud, not for auditing AWS access logs.",
      "AWS Trusted Advisor provides best practice recommendations but does not track or identify specific access attempts to services like Amazon Bedrock."
    ],
    "type": "single",
    "options": [
      "AWS Audit Manager",
      "AWS CloudTrail",
      "Amazon Fraud Detector",
      "AWS Trusted Advisor"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company has developed an ML model for image classification. The company wants to deploy the model to production so that a web application can use the model. The company needs to implement a solution to host the model and serve predictions without managing any of the underlying infrastructure. Which solution will meet these requirements?",
    "explanations": [
      "Amazon SageMaker Serverless Inference allows you to deploy ML models without managing infrastructure. It automatically provisions and scales compute capacity to handle inference requests, making it ideal for web applications that require on-demand predictions.",
      "Amazon CloudFront is a content delivery network (CDN) used for distributing static and dynamic content, not for hosting or serving ML models.",
      "Amazon API Gateway can expose endpoints to invoke a model, but it does not host or serve models by itself. It is typically used in conjunction with a compute backend like SageMaker or Lambda.",
      "AWS Batch is designed for batch computing workloads, not for real-time model inference or hosting."
    ],
    "type": "single",
    "options": [
      "Use Amazon SageMaker Serverless Inference to deploy the model.",
      "Use Amazon CloudFront to deploy the model.",
      "Use Amazon API Gateway to host the model and serve predictions.",
      "Use AWS Batch to host the model and serve predictions."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "An AI company periodically evaluates its systems and processes with the help of independent software vendors (ISVs). The company needs to receive email message notifications when an ISV's compliance reports become available. Which AWS service can the company use to meet this requirement?",
    "explanations": [
      "AWS Artifact provides on-demand access to AWS’s compliance reports and those from independent software vendors (ISVs). It also supports setting up notifications when new compliance documents become available, fulfilling the company's need for email alerts.",
      "AWS Audit Manager helps automate evidence collection for audits but does not focus on third-party compliance report notifications.",
      "AWS Trusted Advisor provides recommendations for security and cost optimization, not compliance reporting from ISVs.",
      "AWS Data Exchange is used for subscribing to and exchanging third-party datasets, but it is not designed for compliance documentation or notifications."
    ],
    "type": "single",
    "options": [
      "AWS Audit Manager",
      "AWS Artifact",
      "AWS Trusted Advisor",
      "AWS Data Exchange"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to use a large language model (LLM) to develop a conversational agent. The company needs to prevent the LLM from being manipulated with common prompt engineering techniques to perform undesirable actions or expose sensitive information. Which action will reduce these risks?",
    "explanations": [
      "Creating a prompt template that teaches the LLM to detect and resist prompt injection or adversarial input is an effective strategy for improving its security posture. It helps the model recognize and reject manipulative inputs designed to override its original instructions.",
      "Increasing the temperature makes the output more random and creative, which can worsen the risk by making the model more unpredictable.",
      "Avoiding unlisted models on SageMaker may reduce general trust risk but does not directly prevent prompt manipulation.",
      "Decreasing the number of input tokens reduces input length but does not inherently protect against prompt injection or adversarial prompt engineering."
    ],
    "type": "single",
    "options": [
      "Create a prompt template that teaches the LLM to detect attack patterns.",
      "Increase the temperature parameter on invocation requests to the LLM.",
      "Avoid using LLMs that are not listed in Amazon SageMaker.",
      "Decrease the number of input tokens on invocations of the LLM."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company is using the Generative AI Security Scoping Matrix to assess security responsibilities for its solutions. The company has identified four different solution scopes based on the matrix. Which solution scope gives the company the MOST ownership of security responsibilities?",
    "explanations": [
      "Building and training a generative AI model from scratch gives the company full control over the model architecture, data pipeline, training, deployment, and monitoring. As a result, the company assumes the greatest share of security responsibilities, including data privacy, model safety, and infrastructure security.",
      "Using a third-party enterprise application with embedded generative AI offers minimal control and therefore minimal security responsibility.",
      "Building an application using a third-party foundation model offers some control over integration and usage, but less than full model ownership.",
      "Fine-tuning a third-party model increases responsibility compared to using it as-is but still relies on third-party components for core behavior and architecture."
    ],
    "type": "single",
    "options": [
      "Using a third-party enterprise application that has embedded generative AI features.",
      "Building an application by using an existing third-party generative AI foundation model (FM).",
      "Refining an existing third-party generative AI foundation model (FM) by fine-tuning the model by using data specific to the business.",
      "Building and training a generative AI model from scratch by using specific data that a customer owns."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "An AI practitioner has a database of animal photos. The AI practitioner wants to automatically identify and categorize the animals in the photos without manual human effort. Which strategy meets these requirements?",
    "explanations": [
      "Object detection is the appropriate strategy for identifying and categorizing objects—such as animals—in images. It can locate and classify multiple animals within each photo automatically.",
      "Anomaly detection is used to find unusual patterns or outliers, not for standard classification of known entities like animals.",
      "Named entity recognition is a natural language processing (NLP) technique for identifying entities in text, not images.",
      "Inpainting is used for filling in missing or corrupted parts of images, not for classification or object recognition."
    ],
    "type": "single",
    "options": [
      "Object detection",
      "Anomaly detection",
      "Named entity recognition",
      "Inpainting"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which Amazon Bedrock pricing model meets these requirements?",
    "explanations": [
      "On-Demand is a core pricing model in Amazon Bedrock that allows users to pay per request (e.g., per input token and output token) without needing to provision infrastructure in advance. It is flexible and ideal for unpredictable or low-volume workloads.",
      "Model customization refers to the process of fine-tuning or continuing pretraining a model and is not a pricing model itself.",
      "Provisioned Throughput is a pricing model used for consistent, high-throughput workloads and requires pre-committed capacity, not ideal for variable usage patterns.",
      "Spot Instance is an EC2 pricing model for spare compute capacity and is not applicable to Amazon Bedrock, which abstracts infrastructure management."
    ],
    "type": "single",
    "options": [
      "On-Demand",
      "Model customization",
      "Provisioned Throughput",
      "Spot Instance"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which AWS service or feature can help an AI development team quickly deploy and consume a foundation model (FM) within the team's VPC?",
    "explanations": [
      "Amazon SageMaker endpoints allow teams to deploy and host machine learning models, including foundation models, in a fully managed environment that integrates with their VPC. This enables secure and low-latency access to the models.",
      "Amazon Personalize is a service for building recommendation systems and is not used for deploying foundation models in a VPC.",
      "Amazon SageMaker JumpStart helps with model discovery and experimentation but does not itself deploy models into a VPC.",
      "PartyRock is an Amazon Bedrock demo environment designed for quick prototyping in a public setting, not suitable for secure VPC deployments."
    ],
    "type": "single",
    "options": [
      "Amazon Personalize",
      "Amazon SageMaker JumpStart",
      "PartyRock, an Amazon Bedrock Playground",
      "Amazon SageMaker endpoints"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "How can companies use large language models (LLMs) securely on Amazon Bedrock?",
    "explanations": [
      "Using clear and specific prompts helps control the behavior of LLMs, and configuring IAM roles and policies with least privilege access ensures that only authorized users and applications can invoke the models. This combination is essential for secure usage of LLMs on Amazon Bedrock.",
      "AWS Audit Manager is used for audit and compliance workflows, not specifically for evaluating or securing LLMs on Bedrock.",
      "Amazon Bedrock does not currently offer automatic model evaluation jobs as a security feature.",
      "Amazon CloudWatch Logs can help monitor usage, but it is not a primary tool for model explainability or bias detection, and it does not directly enforce security controls."
    ],
    "type": "single",
    "options": [
      "Design clear and specific prompts. Configure AWS Identity and Access Management (IAM) roles and policies by using least privilege access.",
      "Enable AWS Audit Manager for automatic model evaluation jobs.",
      "Enable Amazon Bedrock automatic model evaluation jobs.",
      "Use Amazon CloudWatch Logs to make models explainable and to monitor for bias."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has terabytes of data in a database that the company can use for business analysis. The company wants to build an AI-based application that can build a SQL query from input text that employees provide. The employees have minimal experience with technology. Which solution meets these requirements?",
    "explanations": [
      "Generative pre-trained transformers (GPT) are large language models capable of understanding and generating human-like text. They are well-suited for natural language processing tasks such as translating natural language into SQL queries, making them ideal for enabling non-technical employees to interact with databases.",
      "Residual neural networks are typically used in computer vision tasks, not natural language-to-SQL translation.",
      "Support vector machines are traditional machine learning models used for classification and regression, but they are not suitable for generating structured queries from natural language input.",
      "WaveNet is a deep generative model developed for audio waveform generation and is unrelated to natural language understanding or SQL generation."
    ],
    "type": "single",
    "options": [
      "Generative pre-trained transformers (GPT)",
      "Residual neural network",
      "Support vector machine",
      "WaveNet"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company built a deep learning model for object detection and deployed the model to production. Which AI process occurs when the model analyzes a new image to identify objects?",
    "explanations": [
      "Inference is the process where a trained machine learning model is used to make predictions or analyze new data—in this case, analyzing a new image to identify objects.",
      "Training refers to the process of teaching the model using labeled data, which happens before deployment.",
      "Model deployment is the step of making the trained model available in a production environment, not the process of using it.",
      "Bias correction involves identifying and mitigating biases in the model or data, which is unrelated to the task of object detection during runtime."
    ],
    "type": "single",
    "options": [
      "Training",
      "Inference",
      "Model deployment",
      "Bias correction"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An AI practitioner is building a model to generate images of humans in various professions. The AI practitioner discovered that the input data is biased and that specific attributes affect the image generation and create bias in the model. Which technique will solve the problem?",
    "explanations": [
      "Data augmentation for imbalanced classes helps mitigate bias by increasing the diversity and representation of underrepresented groups or categories in the training data. This improves model fairness and reduces skewed outputs.",
      "Model monitoring helps track class distribution over time but does not correct the underlying data imbalance or bias.",
      "Retrieval Augmented Generation (RAG) is used to enhance language model responses with external information and is not a technique for mitigating bias in image generation.",
      "Watermark detection is used to identify the presence of watermarks in images and has no impact on addressing dataset bias."
    ],
    "type": "single",
    "options": [
      "Data augmentation for imbalanced classes",
      "Model monitoring for class distribution",
      "Retrieval Augmented Generation (RAG)",
      "Watermark detection for images"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company is implementing the Amazon Titan foundation model (FM) by using Amazon Bedrock. The company needs to supplement the model by using relevant data from the company's private data sources. Which solution will meet this requirement?",
    "explanations": [
      "Creating an Amazon Bedrock knowledge base allows the company to augment the foundation model's responses with relevant, private company data by connecting to sources like Amazon S3 or internal databases. This supports Retrieval Augmented Generation (RAG).",
      "Using a different FM does not solve the need to incorporate private data unless combined with other mechanisms like knowledge bases.",
      "Choosing a lower temperature affects the randomness of the model's outputs but does not allow it to access or incorporate private company data.",
      "Enabling model invocation logging helps with monitoring and auditing but does not contribute to the model's knowledge or access to private data."
    ],
    "type": "single",
    "options": [
      "Use a different FM.",
      "Choose a lower temperature value.",
      "Create an Amazon Bedrock knowledge base.",
      "Enable model invocation logging."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A medical company is customizing a foundation model (FM) for diagnostic purposes. The company needs the model to be transparent and explainable to meet regulatory requirements. Which solution will meet these requirements?",
    "explanations": [
      "Amazon SageMaker Clarify provides tools to help explain model predictions, detect bias in datasets and models, and generate reports that meet transparency and compliance needs—essential for regulated industries like healthcare.",
      "Amazon Inspector is used for assessing security vulnerabilities in AWS environments, not for model explainability.",
      "Amazon Macie is focused on discovering and protecting sensitive data, not making AI models explainable.",
      "Amazon Rekognition is a computer vision service, and adding custom labels does not directly address model transparency or explainability."
    ],
    "type": "single",
    "options": [
      "Configure the security and compliance by using Amazon Inspector.",
      "Generate simple metrics, reports, and examples by using Amazon SageMaker Clarify.",
      "Encrypt and secure training data by using Amazon Macie.",
      "Gather more data. Use Amazon Rekognition to add custom labels to the data."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to deploy a conversational chatbot to answer customer questions. The chatbot is based on a fine-tuned Amazon SageMaker JumpStart model. The application must comply with multiple regulatory frameworks. Which capabilities can the company show compliance for? (Choose two.)",
    "explanations": [
      "Threat detection is a key component in complying with regulatory frameworks, as it helps monitor and protect the system against malicious activities.",
      "Data protection is essential for regulatory compliance, ensuring that sensitive customer information is securely stored, transmitted, and processed in accordance with data privacy laws.",
      "Auto scaling inference endpoints relate to performance and scalability, not regulatory compliance.",
      "Cost optimization helps with budget management but is not a compliance requirement.",
      "Loosely coupled microservices is an architectural design choice, not a compliance-related capability."
    ],
    "type": "multiple",
    "options": [
      "Auto scaling inference endpoints",
      "Threat detection",
      "Data protection",
      "Cost optimization",
      "Loosely coupled microservices"
    ],
    "correctAnswer": [
      "B",
      "C"
    ]
  },
  {
    "text": "A company is training a foundation model (FM). The company wants to increase the accuracy of the model up to a specific acceptance level. Which solution will meet these requirements?",
    "explanations": [
      "Increasing the number of epochs allows the model to train longer and learn more from the training data, which can improve accuracy up to a desired level—provided it does not lead to overfitting.",
      "Decreasing the batch size may affect model stability and training efficiency but does not directly improve accuracy.",
      "Decreasing the number of epochs would reduce training time and likely lower model accuracy.",
      "Increasing the temperature parameter affects the randomness of the model’s output during inference, not the accuracy during training."
    ],
    "type": "single",
    "options": [
      "Decrease the batch size.",
      "Increase the epochs.",
      "Decrease the epochs.",
      "Increase the temperature parameter."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is building a large language model (LLM) question answering chatbot. The company wants to decrease the number of actions call center employees need to take to respond to customer questions. Which business objective should the company use to evaluate the effect of the LLM chatbot?",
    "explanations": [
      "Average call duration is a relevant business objective because reducing the time it takes for employees to handle customer inquiries directly reflects the chatbot’s effectiveness in streamlining responses and reducing manual effort.",
      "Website engagement rate measures how users interact with a website and is unrelated to call center efficiency.",
      "Corporate social responsibility pertains to ethical and environmental practices, not operational efficiency.",
      "Regulatory compliance is important for legal adherence but does not measure the operational impact of a chatbot on employee workflows."
    ],
    "type": "single",
    "options": [
      "Website engagement rate",
      "Average call duration",
      "Corporate social responsibility",
      "Regulatory compliance"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "Which functionality does Amazon SageMaker Clarify provide?",
    "explanations": [
      "Amazon SageMaker Clarify provides tools to detect potential bias in datasets and models during the data preparation and training phases. It helps ensure fairness and transparency in machine learning workflows.",
      "RAG workflows are not part of SageMaker Clarify’s functionality; they are typically implemented using LLMs and external data sources.",
      "Model quality monitoring in production is handled by Amazon SageMaker Model Monitor, not Clarify.",
      "Documenting model metadata is supported by Amazon SageMaker Model Cards, not Clarify."
    ],
    "type": "single",
    "options": [
      "Integrates a Retrieval Augmented Generation (RAG) workflow",
      "Monitors the quality of ML models in production",
      "Documents critical details about ML models",
      "Identifies potential bias during data preparation"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company is developing a new model to predict the prices of specific items. The model performed well on the training dataset. When the company deployed the model to production, the model's performance decreased significantly. What should the company do to mitigate this problem?",
    "explanations": [
      "Increasing the volume of training data can help the model generalize better to unseen data, which can improve its performance in production environments and reduce overfitting.",
      "Reducing the volume of training data is likely to worsen generalization and further degrade performance.",
      "Adding hyperparameters without proper tuning does not directly solve the issue of poor generalization.",
      "Increasing training time may lead to overfitting on the same dataset and is unlikely to improve real-world performance without addressing data quality and quantity."
    ],
    "type": "single",
    "options": [
      "Reduce the volume of data that is used in training.",
      "Add hyperparameters to the model.",
      "Increase the volume of data that is used in training.",
      "Increase the model training time."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "An ecommerce company wants to build a solution to determine customer sentiments based on written customer reviews of products. Which AWS services meet these requirements? (Choose two.)",
    "explanations": [
      "Amazon Comprehend is a natural language processing (NLP) service that can detect sentiment in text such as customer reviews, making it a direct solution for sentiment analysis.",
      "Amazon Bedrock provides access to foundation models (FMs) that can be used for advanced sentiment analysis through customizable prompts or fine-tuned models.",
      "Amazon Lex is used for building conversational interfaces like chatbots and is not designed for analyzing written text sentiment.",
      "Amazon Polly converts text to lifelike speech and is unrelated to sentiment analysis.",
      "Amazon Rekognition is used for image and video analysis and does not apply to text-based sentiment detection."
    ],
    "type": "multiple",
    "options": [
      "Amazon Lex",
      "Amazon Comprehend",
      "Amazon Polly",
      "Amazon Bedrock",
      "Amazon Rekognition"
    ],
    "correctAnswer": [
      "B",
      "D"
    ]
  },
  {
    "text": "A company wants to use large language models (LLMs) with Amazon Bedrock to develop a chat interface for the company's product manuals. The manuals are stored as PDF files. Which solution meets these requirements MOST cost-effectively?",
    "explanations": [
      "Uploading the PDF documents to an Amazon Bedrock knowledge base enables Retrieval Augmented Generation (RAG), where the relevant content is dynamically retrieved and injected into prompts at runtime. This is highly cost-effective because it avoids passing large amounts of data in every prompt and eliminates the need for expensive fine-tuning.",
      "Adding one PDF file at a time with prompt engineering is limited and may not provide complete or relevant context for all user questions.",
      "Adding all PDF files to each prompt is costly and inefficient due to input token limits and increased inference charges.",
      "Fine-tuning a model on all documents is significantly more expensive and time-consuming, and it reduces flexibility compared to using a knowledge base for retrieval."
    ],
    "type": "single",
    "options": [
      "Use prompt engineering to add one PDF file as context to the user prompt when the prompt is submitted to Amazon Bedrock.",
      "Use prompt engineering to add all the PDF files as context to the user prompt when the prompt is submitted to Amazon Bedrock.",
      "Use all the PDF documents to fine-tune a model with Amazon Bedrock. Use the fine-tuned model to process user prompts.",
      "Upload PDF documents to an Amazon Bedrock knowledge base. Use the knowledge base to provide context when users submit prompts to Amazon Bedrock."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A social media company wants to use a large language model (LLM) for content moderation. The company wants to evaluate the LLM outputs for bias and potential discrimination against specific groups or individuals. Which data source should the company use to evaluate the LLM outputs with the LEAST administrative effort?",
    "explanations": [
      "Benchmark datasets are standardized, pre-labeled datasets specifically designed for evaluating fairness, bias, and discrimination in AI systems. They require minimal administrative overhead because they are already structured for this purpose and can be used out of the box for testing.",
      "User-generated content is unstructured and may require extensive preprocessing and annotation to be useful for bias evaluation.",
      "Moderation logs may provide some insights but typically require significant effort to analyze and are not designed for systematic bias evaluation.",
      "Content moderation guidelines provide policy direction but are not data sources that can be used directly for bias testing."
    ],
    "type": "single",
    "options": [
      "User-generated content",
      "Moderation logs",
      "Content moderation guidelines",
      "Benchmark datasets"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to use a pre-trained generative AI model to generate content for its marketing campaigns. The company needs to ensure that the generated content aligns with the company's brand voice and messaging requirements. Which solution meets these requirements?",
    "explanations": [
      "Creating effective prompts with clear instructions and context is a practical and efficient way to guide a pre-trained generative AI model to produce content that aligns with specific brand voice and messaging goals. This technique is known as prompt engineering.",
      "Optimizing architecture and hyperparameters involves retraining or fine-tuning the model, which is complex and unnecessary when using a capable pre-trained model.",
      "Increasing model complexity is a design-level decision and doesn't directly ensure brand alignment.",
      "Pre-training a new model from scratch using a large, diverse dataset is costly, time-consuming, and unnecessary if a high-quality pre-trained model is already available."
    ],
    "type": "single",
    "options": [
      "Optimize the model's architecture and hyperparameters to improve the model's overall performance.",
      "Increase the model's complexity by adding more layers to the model's architecture.",
      "Create effective prompts that provide clear instructions and context to guide the model's generation.",
      "Select a large, diverse dataset to pre-train a new generative model."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A loan company is building a generative AI-based solution to offer new applicants discounts based on specific business criteria. The company wants to build and use an AI model responsibly to minimize bias that could negatively affect some customers. Which actions should the company take to meet these requirements? (Choose two.)",
    "explanations": [
      "Detecting imbalances or disparities in the data is a critical step in identifying and mitigating bias, ensuring fair treatment across different customer groups.",
      "Evaluating the model's behavior and ensuring transparency helps build trust with stakeholders and aligns with responsible AI practices.",
      "Running the model frequently does not directly address fairness or bias.",
      "ROUGE is a metric for evaluating text summarization and does not ensure model accuracy or fairness in decision-making scenarios.",
      "Inference time relates to performance, not ethical or fair use of AI."
    ],
    "type": "multiple",
    "options": [
      "Detect imbalances or disparities in the data.",
      "Ensure that the model runs frequently.",
      "Evaluate the model's behavior so that the company can provide transparency to stakeholders.",
      "Use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) technique to ensure that the model is 100% accurate.",
      "Ensure that the model's inference time is within the accepted limits."
    ],
    "correctAnswer": [
      "A",
      "C"
    ]
  },
  {
    "text": "A company is using an Amazon Bedrock base model to summarize documents for an internal use case. The company trained a custom model to improve the summarization quality. Which action must the company take to use the custom model through Amazon Bedrock?",
    "explanations": [
      "To use a custom model that was trained using Amazon Bedrock's customization capabilities, the company must grant access to the custom model through Amazon Bedrock. This allows the model to be invoked through the Bedrock API like any other foundation model.",
      "Provisioned Throughput is used to reserve capacity for consistent performance but is not required just to use a custom model.",
      "Amazon SageMaker endpoints are used for hosting models trained in SageMaker, not for Bedrock models.",
      "The SageMaker Model Registry is used to manage model versions in SageMaker and does not integrate with Bedrock's custom model deployment mechanism."
    ],
    "type": "single",
    "options": [
      "Purchase Provisioned Throughput for the custom model.",
      "Deploy the custom model in an Amazon SageMaker endpoint for real-time inference.",
      "Register the model with the Amazon SageMaker Model Registry.",
      "Grant access to the custom model in Amazon Bedrock."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company needs to choose a model from Amazon Bedrock to use internally. The company must identify a model that generates responses in a style that the company's employees prefer. What should the company do to meet these requirements?",
    "explanations": [
      "Evaluating models using a human workforce and custom prompt datasets allows the company to assess whether the model's output aligns with the preferred style and tone of internal users. This approach ensures the evaluation is context-specific and subjective preferences are accurately captured.",
      "Built-in prompt datasets may not reflect the specific tone or communication style that employees prefer.",
      "Public model leaderboards focus on standardized benchmarks and may not capture stylistic or tone-related preferences relevant to the company's internal use case.",
      "InvocationLatency is a performance metric and does not provide insights into the linguistic or stylistic quality of the model's responses."
    ],
    "type": "single",
    "options": [
      "Evaluate the models by using built-in prompt datasets.",
      "Evaluate the models by using a human workforce and custom prompt datasets.",
      "Use public model leaderboards to identify the model.",
      "Use the model InvocationLatency runtime metrics in Amazon CloudWatch when trying models."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A student at a university is copying content from generative AI to write essays. Which challenge of responsible generative AI does this scenario represent?",
    "explanations": [
      "Plagiarism is the correct challenge represented here, as the student is using content generated by AI and presenting it as their own work without proper attribution, which violates academic integrity standards.",
      "Toxicity refers to harmful or offensive content produced by AI, which is not the issue in this scenario.",
      "Hallucinations refer to AI generating false or misleading information, which is not the primary concern here.",
      "Privacy concerns involve unauthorized use or exposure of personal data, which is not applicable in this context."
    ],
    "type": "single",
    "options": [
      "Toxicity",
      "Hallucinations",
      "Plagiarism",
      "Privacy"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company needs to build its own large language model (LLM) based on only the company's private data. The company is concerned about the environmental effect of the training process. Which Amazon EC2 instance type has the LEAST environmental effect when training LLMs?",
    "explanations": [
      "Amazon EC2 Trn series (powered by AWS Trainium chips) is specifically designed for efficient and cost-effective training of machine learning models, especially large language models. These instances are optimized for energy efficiency and sustainability, offering the lowest environmental impact among the listed options.",
      "Amazon EC2 C series is optimized for compute-intensive tasks but not specifically for training LLMs or minimizing environmental impact.",
      "Amazon EC2 G series is designed for graphics-intensive workloads and inference, not for training large language models efficiently.",
      "Amazon EC2 P series (GPU-based) is widely used for training ML models but consumes more energy compared to Trainium-based Trn instances."
    ],
    "type": "single",
    "options": [
      "Amazon EC2 C series",
      "Amazon EC2 G series",
      "Amazon EC2 P series",
      "Amazon EC2 Trn series"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to build an interactive application for children that generates new stories based on classic stories. The company wants to use Amazon Bedrock and needs to ensure that the results and topics are appropriate for children. Which AWS service or feature will meet these requirements?",
    "explanations": [
      "Guardrails for Amazon Bedrock allow developers to set content filters and safety controls to ensure outputs generated by foundation models are appropriate for specific audiences, such as children. This helps prevent the generation of harmful, unsafe, or age-inappropriate content.",
      "Amazon Rekognition is used for analyzing images and videos, not for moderating or filtering text generated by LLMs.",
      "Amazon Bedrock playgrounds are environments for testing and experimenting with models but do not provide built-in safety controls.",
      "Agents for Amazon Bedrock help orchestrate multistep tasks using foundation models but are not specifically designed to enforce content appropriateness or safety."
    ],
    "type": "single",
    "options": [
      "Amazon Rekognition",
      "Amazon Bedrock playgrounds",
      "Guardrails for Amazon Bedrock",
      "Agents for Amazon Bedrock"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is building an application that needs to generate synthetic data that is based on existing data. Which type of model can the company use to meet this requirement?",
    "explanations": [
      "Generative adversarial networks (GANs) are specifically designed to generate realistic synthetic data by learning from existing datasets. They are widely used in applications such as image synthesis, data augmentation, and more.",
      "XGBoost is a gradient boosting algorithm used for supervised learning tasks like classification and regression, not for generating synthetic data.",
      "Residual neural networks (ResNets) are primarily used for deep learning tasks in computer vision, not for generating synthetic data.",
      "WaveNet is a generative model for raw audio waveforms, not a general-purpose model for synthetic data generation across different domains."
    ],
    "type": "single",
    "options": [
      "Generative adversarial network (GAN)",
      "XGBoost",
      "Residual neural network",
      "WaveNet"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A digital devices company wants to predict customer demand for memory hardware. The company does not have coding experience or knowledge of ML algorithms and needs to develop a data-driven predictive model. The company needs to perform analysis on internal data and external data. Which solution will meet these requirements?",
    "explanations": [
      "Amazon SageMaker Canvas is a no-code machine learning service that allows users without programming or ML expertise to build, train, and deploy predictive models using an intuitive graphical interface. It supports importing internal and external data and generating forecasts easily.",
      "Amazon SageMaker built-in algorithms and Data Wrangler require some familiarity with ML workflows and basic coding, making them less suitable for users without technical experience.",
      "Amazon Personalize is tailored for recommendation systems, not general demand forecasting, and it also requires more technical setup.",
      "SageMaker built-in algorithms and Amazon S3 integration are useful for advanced users, not for those with no coding or ML knowledge."
    ],
    "type": "single",
    "options": [
      "Store the data in Amazon S3. Create ML models and demand forecast predictions by using Amazon SageMaker built-in algorithms that use the data from Amazon S3.",
      "Import the data into Amazon SageMaker Data Wrangler. Create ML models and demand forecast predictions by using SageMaker built-in algorithms.",
      "Import the data into Amazon SageMaker Data Wrangler. Build ML models and demand forecast predictions by using an Amazon Personalize Trending-Now recipe.",
      "Import the data into Amazon SageMaker Canvas. Build ML models and demand forecast predictions by selecting the values in the data from SageMaker Canvas."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company has installed a security camera. The company uses an ML model to evaluate the security camera footage for potential thefts. The company has discovered that the model disproportionately flags people who are members of a specific ethnic group. Which type of bias is affecting the model output?",
    "explanations": [
      "Sampling bias occurs when the training data is not representative of the entire population. In this case, if the training data overrepresented or misrepresented individuals from a specific ethnic group, it could lead the model to unfairly and disproportionately flag members of that group.",
      "Measurement bias refers to errors in how data is collected or labeled, but it is not specifically tied to the overrepresentation of a group.",
      "Observer bias involves subjective interpretation by a human observer, which is not the case with automated ML predictions.",
      "Confirmation bias refers to the tendency to interpret data in a way that confirms preexisting beliefs, usually in human judgment rather than algorithmic processing."
    ],
    "type": "single",
    "options": [
      "Measurement bias",
      "Sampling bias",
      "Observer bias",
      "Confirmation bias"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is building a customer service chatbot. The company wants the chatbot to improve its responses by learning from past interactions and online resources. Which AI learning strategy provides this self-improvement capability?",
    "explanations": [
      "Reinforcement learning allows a model to improve over time by receiving feedback in the form of rewards or penalties. In the case of a chatbot, it can be trained to optimize its responses based on positive customer feedback, leading to continuous self-improvement.",
      "Supervised learning with labeled responses helps with initial training but does not support ongoing improvement from real-time interactions without manual updates.",
      "Unsupervised learning can help understand patterns in data but does not involve learning from feedback to improve behavior.",
      "A continuously updated FAQ can enhance relevance, but supervised learning on static data doesn't enable dynamic self-improvement based on user interaction."
    ],
    "type": "single",
    "options": [
      "Supervised learning with a manually curated dataset of good responses and bad responses",
      "Reinforcement learning with rewards for positive customer feedback",
      "Unsupervised learning to find clusters of similar customer inquiries",
      "Supervised learning with a continuously updated FAQ database"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An AI practitioner has built a deep learning model to classify the types of materials in images. The AI practitioner now wants to measure the model performance. Which metric will help the AI practitioner evaluate the performance of the model?",
    "explanations": [
      "A confusion matrix is the most suitable metric for evaluating the performance of a classification model. It provides detailed information about true positives, false positives, true negatives, and false negatives, helping to assess the accuracy and reliability of predictions.",
      "A correlation matrix shows relationships between variables but does not evaluate classification performance.",
      "R2 score is used for regression tasks, not classification.",
      "Mean squared error (MSE) is also a regression metric and not appropriate for evaluating classification models."
    ],
    "type": "single",
    "options": [
      "Confusion matrix",
      "Correlation matrix",
      "R2 score",
      "Mean squared error (MSE)"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has built a chatbot that can respond to natural language questions with images. The company wants to ensure that the chatbot does not return inappropriate or unwanted images. Which solution will meet these requirements?",
    "explanations": [
      "Implementing moderation APIs is the most effective and immediate way to filter and block inappropriate or unwanted images before they are shown to users. These APIs can automatically detect explicit or harmful content.",
      "Retraining the model with a general public dataset does not guarantee the removal of inappropriate content and may not align with the specific safety requirements.",
      "Model validation ensures general performance accuracy but is not focused on detecting or preventing inappropriate content in outputs.",
      "Automating user feedback helps with long-term improvement but is not sufficient as a proactive control for content safety."
    ],
    "type": "single",
    "options": [
      "Implement moderation APIs.",
      "Retrain the model with a general public dataset.",
      "Perform model validation.",
      "Automate user feedback integration."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "An AI practitioner is using an Amazon Bedrock base model to summarize session chats from the customer service department. The AI practitioner wants to store invocation logs to monitor model input and output data. Which strategy should the AI practitioner use?",
    "explanations": [
      "Enabling invocation logging in Amazon Bedrock allows practitioners to capture and store input and output data from model invocations, making it the correct strategy for monitoring model behavior and performance.",
      "AWS CloudTrail tracks API calls for auditing but does not capture the detailed input/output content of model invocations.",
      "AWS Audit Manager is used for compliance and audit readiness, not for logging model interactions.",
      "Amazon EventBridge is used for event-driven architectures and does not store model input/output logs by default."
    ],
    "type": "single",
    "options": [
      "Configure AWS CloudTrail as the logs destination for the model.",
      "Enable invocation logging in Amazon Bedrock.",
      "Configure AWS Audit Manager as the logs destination for the model.",
      "Configure model invocation logging in Amazon EventBridge."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is building an ML model to analyze archived data. The company must perform inference on large datasets that are multiple GBs in size. The company does not need to access the model predictions immediately. Which Amazon SageMaker inference option will meet these requirements?",
    "explanations": [
      "Batch transform is designed for offline inference on large datasets. It is ideal for scenarios where you don’t need real-time results and want to process data in bulk. It supports large files and can handle inputs stored in Amazon S3.",
      "Real-time inference is suitable for low-latency, on-demand predictions but is not efficient or cost-effective for large datasets or offline use cases.",
      "Serverless inference is optimized for lightweight, intermittent inference requests and has payload size limits that make it unsuitable for multi-GB datasets.",
      "Asynchronous inference supports large payloads and longer processing times, but it is more suitable for near real-time use cases where results are needed shortly after processing rather than true batch processing."
    ],
    "type": "single",
    "options": [
      "Batch transform",
      "Real-time inference",
      "Serverless inference",
      "Asynchronous inference"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which term describes the numerical representations of real-world objects and concepts that AI and natural language processing (NLP) models use to improve understanding of textual information?",
    "explanations": [
      "Embeddings are numerical representations of real-world concepts and words in a continuous vector space. They help AI and NLP models understand relationships, similarities, and meanings in textual data.",
      "Tokens are individual pieces of input text (like words or subwords), not numerical representations.",
      "Models refer to the entire machine learning or AI system, not the specific representations of data within them.",
      "Binaries typically refer to compiled executable files or binary data formats, unrelated to semantic understanding in NLP."
    ],
    "type": "single",
    "options": [
      "Embeddings",
      "Tokens",
      "Models",
      "Binaries"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A research company implemented a chatbot by using a foundation model (FM) from Amazon Bedrock. The chatbot searches for answers to questions from a large database of research papers. After multiple prompt engineering attempts, the company notices that the FM is performing poorly because of the complex scientific terms in the research papers. How can the company improve the performance of the chatbot?",
    "explanations": [
      "Domain adaptation fine-tuning allows the foundation model to learn terminology and language patterns specific to scientific literature, improving its understanding and accuracy when answering domain-specific questions.",
      "Few-shot prompting can help guide the model's behavior, but it may not be sufficient if the model lacks understanding of specialized vocabulary.",
      "Changing inference parameters (like temperature or max tokens) affects output style and randomness but does not enhance domain understanding.",
      "Removing complex scientific terms would eliminate essential context and information, reducing the quality and relevance of answers."
    ],
    "type": "single",
    "options": [
      "Use few-shot prompting to define how the FM can answer the questions.",
      "Use domain adaptation fine-tuning to adapt the FM to complex scientific terms.",
      "Change the FM inference parameters.",
      "Clean the research paper data to remove complex scientific terms."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company needs the LLM to produce more consistent responses to the same input prompt. Which adjustment to an inference parameter should the company make to meet these requirements?",
    "explanations": [
      "Decreasing the temperature value reduces randomness in the model's output, leading to more consistent and deterministic responses for the same input prompt. This is ideal for tasks like sentiment analysis that require reliability.",
      "Increasing the temperature value would make the output more random and less consistent.",
      "Decreasing the length of output tokens limits response length but does not improve consistency.",
      "Increasing the maximum generation length allows for longer outputs but does not directly affect response consistency."
    ],
    "type": "single",
    "options": [
      "Decrease the temperature value.",
      "Increase the temperature value.",
      "Decrease the length of output tokens.",
      "Increase the maximum generation length."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to develop a large language model (LLM) application by using Amazon Bedrock and customer data that is uploaded to Amazon S3. The company's security policy states that each team can access data for only the team's own customers. Which solution will meet these requirements?",
    "explanations": [
      "Creating a separate Amazon Bedrock custom service role for each team with scoped access to only their specific customer data ensures fine-grained access control and aligns with the company’s security policy. This prevents cross-team access to customer information.",
      "Asking teams to specify customer names manually in requests does not enforce security and can lead to data exposure if misused.",
      "Redacting personal data may reduce sensitivity, but it does not fulfill the requirement to restrict access by team.",
      "Creating a single Bedrock role with full S3 access and using IAM roles separately does not restrict what the Bedrock service itself can access, violating the least privilege principle."
    ],
    "type": "single",
    "options": [
      "Create an Amazon Bedrock custom service role for each team that has access to only the team's customer data.",
      "Create a custom service role that has Amazon S3 access. Ask teams to specify the customer name on each Amazon Bedrock request.",
      "Redact personal data in Amazon S3. Update the S3 bucket policy to allow team access to customer data.",
      "Create one Amazon Bedrock role that has full Amazon S3 access. Create IAM roles for each team that have access to only each team's customer folders."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A medical company deployed a disease detection model on Amazon Bedrock. To comply with privacy policies, the company wants to prevent the model from including personal patient information in its responses. The company also wants to receive notification when policy violations occur. Which solution meets these requirements?",
    "explanations": [
      "Guardrails for Amazon Bedrock allow you to filter and block specific types of content, such as personally identifiable information (PII), ensuring compliance with privacy policies. Combined with Amazon CloudWatch alarms, this solution enables real-time notifications for policy violations.",
      "Amazon Macie is designed for scanning S3 data for sensitive information, not for real-time model output monitoring or filtering.",
      "AWS CloudTrail logs API calls and activities but does not analyze or filter model output content.",
      "Amazon SageMaker Model Monitor is used for monitoring model performance and data drift, not for content filtering or privacy compliance in Bedrock models."
    ],
    "type": "single",
    "options": [
      "Use Amazon Macie to scan the model's output for sensitive data and set up alerts for potential violations.",
      "Configure AWS CloudTrail to monitor the model's responses and create alerts for any detected personal information.",
      "Use Guardrails for Amazon Bedrock to filter content. Set up Amazon CloudWatch alarms for notification of policy violations.",
      "Implement Amazon SageMaker Model Monitor to detect data drift and receive alerts when model quality degrades."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company manually reviews all submitted resumes in PDF format. As the company grows, the company expects the volume of resumes to exceed the company's review capacity. The company needs an automated system to convert the PDF resumes into plain text format for additional processing. Which AWS service meets this requirement?",
    "explanations": [
      "Amazon Textract is designed to extract text, forms, and tables from scanned documents, including PDFs. It is the most suitable service for converting resume documents into plain text for further automated processing.",
      "Amazon Personalize is used for building recommendation systems and is unrelated to document processing.",
      "Amazon Lex is a service for building conversational interfaces and chatbots, not for extracting text from documents.",
      "Amazon Transcribe is used for converting spoken audio into text and is not applicable to text extraction from PDF files."
    ],
    "type": "single",
    "options": [
      "Amazon Textract",
      "Amazon Personalize",
      "Amazon Lex",
      "Amazon Transcribe"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "An education provider is building a question and answer application that uses a generative AI model to explain complex concepts. The education provider wants to automatically change the style of the model response depending on who is asking the question. The education provider will give the model the age range of the user who has asked the question. Which solution meets these requirements with the LEAST implementation effort?",
    "explanations": [
      "Adding a role description to the prompt context is a simple and effective method for tailoring responses based on age. For example, including 'Explain this concept to a 10-year-old' in the prompt leverages the model's understanding without needing retraining or complex logic, making it the most efficient solution.",
      "Fine-tuning requires significant resources, time, and data preparation, making it more complex and costly for this use case.",
      "Chain-of-thought reasoning helps the model explain step-by-step but does not dynamically tailor style based on user characteristics unless prompted.",
      "Summarizing responses post-generation adds processing steps and may not adequately control language complexity or tone for different age groups."
    ],
    "type": "single",
    "options": [
      "Fine-tune the model by using additional training data that is representative of the various age ranges that the application will support.",
      "Add a role description to the prompt context that instructs the model of the age range that the response should target.",
      "Use chain-of-thought reasoning to deduce the correct style and complexity for a response suitable for that user.",
      "Summarize the response text depending on the age of the user so that younger users receive shorter responses."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "Which strategy evaluates the accuracy of a foundation model (FM) that is used in image classification tasks?",
    "explanations": [
      "Measuring the model's accuracy against a predefined benchmark dataset is the standard method for evaluating the performance of an image classification model. It allows consistent comparison between model predictions and ground truth labels.",
      "Calculating the cost of resources relates to operational efficiency, not model accuracy.",
      "Counting neural network layers provides architectural information but does not indicate how well the model performs.",
      "Assessing color accuracy is relevant for image processing or rendering tasks, not for classification accuracy."
    ],
    "type": "single",
    "options": [
      "Calculate the total cost of resources used by the model.",
      "Measure the model's accuracy against a predefined benchmark dataset.",
      "Count the number of layers in the neural network.",
      "Assess the color accuracy of images processed by the model."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An accounting firm wants to implement a large language model (LLM) to automate document processing. The firm must proceed responsibly to avoid potential harms. What should the firm do when developing and deploying the LLM? (Choose two.)",
    "explanations": [
      "Including fairness metrics ensures that the model is evaluated for potential discriminatory behavior, which is critical for responsible AI deployment, especially in sensitive fields like accounting.",
      "Modifying the training data to mitigate bias is a proactive step in building ethical AI systems and helps reduce the risk of harmful or unfair outputs.",
      "Adjusting the temperature parameter affects output creativity and randomness but does not directly relate to responsible AI practices such as fairness or bias mitigation.",
      "Avoiding overfitting is important for general model performance but does not specifically address responsible deployment concerns like fairness or bias.",
      "Prompt engineering improves usability and relevance but is not a substitute for ethical safeguards in model development."
    ],
    "type": "multiple",
    "options": [
      "Include fairness metrics for model evaluation.",
      "Adjust the temperature parameter of the model.",
      "Modify the training data to mitigate bias.",
      "Avoid overfitting on the training data.",
      "Apply prompt engineering techniques."
    ],
    "correctAnswer": [
      "A",
      "C"
    ]
  },
  {
    "text": "A company is building an ML model. The company collected new data and analyzed the data by creating a correlation matrix, calculating statistics, and visualizing the data. Which stage of the ML pipeline is the company currently in?",
    "explanations": [
      "Exploratory data analysis (EDA) involves summarizing and visualizing datasets to understand their main characteristics, often using statistics, correlation matrices, and plots. This helps guide decisions about data cleaning, feature selection, and model choice.",
      "Data pre-processing typically involves cleaning data, handling missing values, and formatting it for use in a model, which goes beyond just analysis.",
      "Feature engineering refers to creating new input features or transforming existing ones to improve model performance.",
      "Hyperparameter tuning is part of the model training process and involves optimizing model configuration, not data understanding."
    ],
    "type": "single",
    "options": [
      "Data pre-processing",
      "Feature engineering",
      "Exploratory data analysis",
      "Hyperparameter tuning"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company has documents that are missing some words because of a database error. The company wants to build an ML model that can suggest potential words to fill in the missing text. Which type of model meets this requirement?",
    "explanations": [
      "BERT-based models are designed to understand context in text and are particularly good at masked language modeling, where a model predicts missing words in a sentence. This makes BERT ideal for filling in gaps caused by missing words.",
      "Topic modeling identifies themes or topics in a document but does not predict missing words.",
      "Clustering models group similar data points and are not used for language generation or completion.",
      "Prescriptive ML models suggest actions based on predictions, which is unrelated to filling in missing text."
    ],
    "type": "single",
    "options": [
      "Topic modeling",
      "Clustering models",
      "Prescriptive ML models",
      "BERT-based models"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to display the total sales for its top-selling products across various retail locations in the past 12 months. Which AWS solution should the company use to automate the generation of graphs?",
    "explanations": [
      "Amazon Q in Amazon QuickSight enables users to ask business-related questions in natural language and automatically generates visualizations like graphs based on the underlying data. It is specifically designed for business intelligence and analytics use cases like sales performance reporting.",
      "Amazon Q in Amazon EC2 does not exist as a product and is not related to visualization.",
      "Amazon Q Developer is intended to assist developers with code generation and development tasks, not business data visualization.",
      "Amazon Q in AWS Chatbot facilitates DevOps-related tasks and notifications, not generating graphs for business data analysis."
    ],
    "type": "single",
    "options": [
      "Amazon Q in Amazon EC2",
      "Amazon Q Developer",
      "Amazon Q in Amazon QuickSight",
      "Amazon Q in AWS Chatbot"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is building a chatbot to improve user experience. The company is using a large language model (LLM) from Amazon Bedrock for intent detection. The company wants to use few-shot learning to improve intent detection accuracy. Which additional data does the company need to meet these requirements?",
    "explanations": [
      "Few-shot learning involves providing the LLM with a few examples of input-output pairs within the prompt. For intent detection, the correct examples are user messages and their corresponding user intents. This allows the model to learn how to classify new user inputs based on those examples.",
      "Pairs of chatbot responses and user intents do not train the model to detect intent from the user's input.",
      "Pairs of user messages and chatbot responses are more relevant to response generation, not intent detection.",
      "Pairs of user intents and chatbot responses reverse the input-output structure needed for intent detection."
    ],
    "type": "single",
    "options": [
      "Pairs of chatbot responses and correct user intents",
      "Pairs of user messages and correct chatbot responses",
      "Pairs of user messages and correct user intents",
      "Pairs of user intents and correct chatbot responses"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is using few-shot prompting on a base model that is hosted on Amazon Bedrock. The model currently uses 10 examples in the prompt. The model is invoked once daily and is performing well. The company wants to lower the monthly cost. Which solution will meet these requirements?",
    "explanations": [
      "Decreasing the number of tokens in the prompt reduces the number of input tokens processed by the model, directly lowering the cost per invocation, which is ideal since the model is only used once per day and is already performing well.",
      "Customizing the model with fine-tuning involves upfront costs and is better suited for high-frequency or complex use cases.",
      "Increasing the number of tokens in the prompt would increase the cost, which contradicts the goal of cost reduction.",
      "Provisioned Throughput is intended for high-volume, low-latency use cases and would not be cost-effective for once-daily invocations."
    ],
    "type": "single",
    "options": [
      "Customize the model by using fine-tuning.",
      "Decrease the number of tokens in the prompt.",
      "Increase the number of tokens in the prompt.",
      "Use Provisioned Throughput."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An AI practitioner is using a large language model (LLM) to create content for marketing campaigns. The generated content sounds plausible and factual but is incorrect. Which problem is the LLM having?",
    "explanations": [
      "Hallucination refers to a common issue in large language models where the model generates content that appears accurate and believable but is actually incorrect or fabricated. This is the described problem.",
      "Data leakage occurs when the model has access to information during training that it shouldn't, leading to overly optimistic performance—unrelated to generating incorrect but plausible text.",
      "Overfitting happens when a model memorizes the training data and fails to generalize, which can lead to incorrect outputs but typically results in poor performance on new data rather than generating plausible misinformation.",
      "Underfitting means the model has not learned enough from the training data, generally leading to poor or overly simplistic predictions, not misleadingly confident outputs."
    ],
    "type": "single",
    "options": [
      "Data leakage",
      "Hallucination",
      "Overfitting",
      "Underfitting"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An AI practitioner trained a custom model on Amazon Bedrock by using a training dataset that contains confidential data. The AI practitioner wants to ensure that the custom model does not generate inference responses based on confidential data. How should the AI practitioner prevent responses based on confidential data?",
    "explanations": [
      "To prevent the model from generating responses based on confidential data, the best approach is to remove that data from the training dataset and retrain the model. Once a model has been trained, the information it learned is embedded in its weights, and it can potentially reproduce that information during inference.",
      "Dynamic data masking operates on structured data at query time and is not applicable to text generated by a language model.",
      "Encrypting inference responses does not prevent the disclosure of sensitive information—it only secures the transmission or storage of the response.",
      "Encrypting the model with AWS KMS protects the model at rest but does not prevent the model from outputting sensitive content learned during training."
    ],
    "type": "single",
    "options": [
      "Delete the custom model. Remove the confidential data from the training dataset. Retrain the custom model.",
      "Mask the confidential data in the inference responses by using dynamic data masking.",
      "Encrypt the confidential data in the inference responses by using Amazon SageMaker.",
      "Encrypt the confidential data in the custom model by using AWS Key Management Service (AWS KMS)."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has built a solution by using generative AI. The solution uses large language models (LLMs) to translate training manuals from English into other languages. The company wants to evaluate the accuracy of the solution by examining the text generated for the manuals. Which model evaluation strategy meets these requirements?",
    "explanations": [
      "BLEU (Bilingual Evaluation Understudy) is a widely used metric for evaluating the accuracy of machine translation systems. It compares the overlap of n-grams between the generated translation and one or more reference translations.",
      "RMSE (Root Mean Squared Error) is used for regression tasks and measures the average magnitude of prediction errors, which is not applicable for evaluating text translations.",
      "ROUGE is typically used for evaluating text summarization, focusing on recall of n-gram overlaps, not translation accuracy.",
      "F1 score is commonly used in classification tasks to measure the balance between precision and recall, not for evaluating translation quality."
    ],
    "type": "single",
    "options": [
      "Bilingual Evaluation Understudy (BLEU)",
      "Root mean squared error (RMSE)",
      "Recall-Oriented Understudy for Gisting Evaluation (ROUGE)",
      "F1 score"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A large retailer receives thousands of customer support inquiries about products every day. The customer support inquiries need to be processed and responded to quickly. The company wants to implement Agents for Amazon Bedrock. What are the key benefits of using Amazon Bedrock agents that could help this retailer?",
    "explanations": [
      "Agents for Amazon Bedrock allow you to define goals, create workflows, and connect foundation models to data sources and APIs to automate actions. This is especially beneficial for automating repetitive customer support tasks and orchestrating complex workflows, helping improve response times and customer satisfaction.",
      "Amazon Bedrock agents do not generate custom FMs; they work with existing foundation models to automate processes.",
      "While agents can interact with tools and APIs, they do not inherently consolidate outputs from multiple FMs simultaneously.",
      "Agents do not automatically select foundation models based on criteria or metrics; model selection is typically a manual or configuration-driven process."
    ],
    "type": "single",
    "options": [
      "Generation of custom foundation models (FMs) to predict customer needs",
      "Automation of repetitive tasks and orchestration of complex workflows",
      "Automatically calling multiple foundation models (FMs) and consolidating the results",
      "Selecting the foundation model (FM) based on predefined criteria and metrics"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "Which option is a benefit of ongoing pre-training when fine-tuning a foundation model (FM)?",
    "explanations": [
      "Ongoing pre-training allows a foundation model to continue learning from additional relevant data, which helps improve its performance over time by making it more knowledgeable and context-aware for downstream tasks.",
      "Decreasing model complexity is not a goal of ongoing pre-training—it may even increase complexity as the model becomes more specialized.",
      "Ongoing pre-training typically increases training time, rather than decreasing it.",
      "Optimizing inference time is related to model deployment and architecture, not the pre-training process."
    ],
    "type": "single",
    "options": [
      "Helps decrease the model's complexity",
      "Improves model performance over time",
      "Decreases the training time requirement",
      "Optimizes model inference time"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "What are tokens in the context of generative AI models?",
    "explanations": [
      "Tokens are the fundamental units of input and output for generative AI models. They can represent words, subwords, characters, or other text segments, depending on the model's tokenization strategy. Models process text as sequences of tokens.",
      "Mathematical representations of words or concepts are called embeddings, not tokens.",
      "Pre-trained weights are part of the model's internal parameters, not tokens.",
      "Prompts are instructions or inputs provided to the model, but they are composed of tokens—not tokens themselves."
    ],
    "type": "single",
    "options": [
      "Tokens are the basic units of input and output that a generative AI model operates on, representing words, subwords, or other linguistic units.",
      "Tokens are the mathematical representations of words or concepts used in generative AI models.",
      "Tokens are the pre-trained weights of a generative AI model that are fine-tuned for specific tasks.",
      "Tokens are the specific prompts or instructions given to a generative AI model to generate output."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to assess the costs that are associated with using a large language model (LLM) to generate inferences. The company wants to use Amazon Bedrock to build generative AI applications. Which factor will drive the inference costs?",
    "explanations": [
      "Inference costs on Amazon Bedrock are primarily driven by the number of tokens consumed during a model invocation. This includes both input tokens (from the prompt) and output tokens (generated by the model).",
      "Temperature value influences the randomness of responses but does not affect pricing.",
      "The amount of data used to train the model impacts training costs, not inference costs.",
      "Total training time is a factor in model development cost, not in ongoing inference usage or pricing."
    ],
    "type": "single",
    "options": [
      "Number of tokens consumed",
      "Temperature value",
      "Amount of data used to train the LLM",
      "Total training time"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company is using Amazon SageMaker Studio notebooks to build and train ML models. The company stores the data in an Amazon S3 bucket. The company needs to manage the flow of data from Amazon S3 to SageMaker Studio notebooks. Which solution will meet this requirement?",
    "explanations": [
      "Configuring Amazon SageMaker to use a VPC with an S3 endpoint allows secure and efficient access to S3 data from within SageMaker Studio notebooks without sending traffic over the public internet. This setup is ideal for managing the data flow between S3 and SageMaker.",
      "Amazon Inspector is a security assessment service and does not manage data flow.",
      "Amazon Macie is used for discovering and protecting sensitive data, not managing or facilitating data movement.",
      "S3 Glacier Deep Archive is intended for long-term storage of infrequently accessed data and is not suitable for training ML models due to high retrieval latency and cost."
    ],
    "type": "single",
    "options": [
      "Use Amazon Inspector to monitor SageMaker Studio.",
      "Use Amazon Macie to monitor SageMaker Studio.",
      "Configure SageMaker to use a VPC with an S3 endpoint.",
      "Configure SageMaker to use S3 Glacier Deep Archive."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company has a foundation model (FM) that was customized by using Amazon Bedrock to answer customer queries about products. The company wants to validate the model's responses to new types of queries. The company needs to upload a new dataset that Amazon Bedrock can use for validation. Which AWS service meets these requirements?",
    "explanations": [
      "Amazon S3 is the correct service for uploading datasets to use with Amazon Bedrock for model validation or customization. Bedrock integrates directly with Amazon S3 for accessing training and validation datasets.",
      "Amazon EBS provides block storage for EC2 instances and is not used for direct integration with Bedrock.",
      "Amazon EFS is used for file storage across multiple EC2 instances and is not directly supported for dataset uploads in Amazon Bedrock workflows.",
      "AWS Snowcone is a physical edge device used for offline data transfer, not for uploading datasets in real-time to validate foundation models."
    ],
    "type": "single",
    "options": [
      "Amazon S3",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon Elastic File System (Amazon EFS)",
      "AWS Snowcone"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which prompting attack directly exposes the configured behavior of a large language model (LLM)?",
    "explanations": [
      "Extracting the prompt template is a prompting attack that attempts to reveal the underlying instructions or configuration given to the LLM, thereby exposing its intended behavior, constraints, or role.",
      "Prompted persona switches involve tricking the model into changing its assumed identity or role but do not directly expose the configured behavior.",
      "Exploiting friendliness and trust attempts to manipulate the model through socially engineered prompts, not to uncover hidden instructions.",
      "Ignoring the prompt template refers to the model not following instructions but doesn't imply revealing the underlying configuration."
    ],
    "type": "single",
    "options": [
      "Prompted persona switches",
      "Exploiting friendliness and trust",
      "Ignoring the prompt template",
      "Extracting the prompt template"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to use Amazon Bedrock. The company needs to review which security aspects the company is responsible for when using Amazon Bedrock. Which security aspect will the company be responsible for?",
    "explanations": [
      "In the shared responsibility model for AWS services, AWS is responsible for the security 'of' the cloud (such as infrastructure and service maintenance), while customers are responsible for the security 'in' the cloud. This includes securing their data in transit and at rest when using services like Amazon Bedrock.",
      "Patching and updating Amazon Bedrock is managed by AWS.",
      "Protecting the infrastructure that hosts Amazon Bedrock is also AWS’s responsibility.",
      "Amazon Bedrock is a fully managed service; customers do not provision it within their own networks but rather access it via AWS."
    ],
    "type": "single",
    "options": [
      "Patching and updating the versions of Amazon Bedrock",
      "Protecting the infrastructure that hosts Amazon Bedrock",
      "Securing the company's data in transit and at rest",
      "Provisioning Amazon Bedrock within the company network"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A social media company wants to use a large language model (LLM) to summarize messages. The company has chosen a few LLMs that are available on Amazon SageMaker JumpStart. The company wants to compare the generated output toxicity of these models. Which strategy gives the company the ability to evaluate the LLMs with the LEAST operational overhead?",
    "explanations": [
      "Automatic model evaluation allows for scalable and low-overhead assessment of model outputs, including toxicity detection, by using predefined metrics and automated tools. It is the most efficient option for evaluating multiple models with minimal human involvement.",
      "Crowd-sourced evaluation involves managing a large group of evaluators and introduces significant operational complexity.",
      "Model evaluation with human workers is accurate but requires coordination, time, and cost, leading to higher overhead.",
      "RLHF is a training approach that requires significant resources and is not suitable for straightforward model comparison tasks."
    ],
    "type": "single",
    "options": [
      "Crowd-sourced evaluation",
      "Automatic model evaluation",
      "Model evaluation with human workers",
      "Reinforcement learning from human feedback (RLHF)"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is testing the security of a foundation model (FM). During testing, the company wants to get around the safety features and make harmful content. Which security technique is this an example of?",
    "explanations": [
      "Jailbreak refers to a technique used to bypass the built-in safety mechanisms of a foundation model in order to produce restricted or harmful content. This is a common test during security evaluations to assess the robustness of model safeguards.",
      "Fuzzing training data involves providing malformed or unexpected data to uncover vulnerabilities but is not focused on bypassing safety filters.",
      "Denial of service (DoS) is an attack that aims to make a system unavailable, not to bypass content restrictions.",
      "Penetration testing with authorization is a broader security practice to identify system vulnerabilities, but jailbreak specifically refers to evading model safety constraints."
    ],
    "type": "single",
    "options": [
      "Fuzzing training data to find vulnerabilities",
      "Denial of service (DoS)",
      "Penetration testing with authorization",
      "Jailbreak"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company needs to use Amazon SageMaker for model training and inference. The company must comply with regulatory requirements to run SageMaker jobs in an isolated environment without internet access. Which solution will meet these requirements?",
    "explanations": [
      "Network isolation in Amazon SageMaker ensures that training and inference jobs run in an environment without internet access. This is essential for meeting regulatory or compliance requirements involving data confidentiality and secure processing.",
      "SageMaker Experiments is used for organizing, tracking, and comparing ML experiments but does not enforce network isolation.",
      "Encrypting data at rest is important for data protection but does not address the need for an isolated, internet-restricted environment.",
      "IAM roles are necessary for access control but do not ensure network isolation on their own."
    ],
    "type": "single",
    "options": [
      "Run SageMaker training and inference by using SageMaker Experiments.",
      "Run SageMaker training and Inference by using network Isolation.",
      "Encrypt the data at rest by using encryption for SageMaker geospatial capabilities.",
      "Associate appropriate AWS Identity and Access Management (IAM) roles with the SageMaker jobs."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "An ML research team develops custom ML models. The model artifacts are shared with other teams for integration into products and services. The ML team retains the model training code and data. The ML team wants to build a mechanism that the ML team can use to audit models. Which solution should the ML team use when publishing the custom ML models?",
    "explanations": [
      "Amazon SageMaker Model Cards provide a standardized and structured way to document key details about ML models, including their intended use cases, training data, evaluation metrics, and ethical considerations. They help with auditing and model governance.",
      "Storing documents in Amazon S3 lacks structure and doesn't provide native integration with SageMaker or audit workflows.",
      "AWS AI Service Cards are maintained by AWS for their own AI services and are not a tool for documenting custom models.",
      "Committing training scripts to a Git repository supports version control but does not provide the transparency, documentation, or auditing structure that model cards offer."
    ],
    "type": "single",
    "options": [
      "Create documents with the relevant information. Store the documents in Amazon S3.",
      "Use AWS AI Service Cards for transparency and understanding models.",
      "Create Amazon SageMaker Model Cards with intended uses and training and inference details.",
      "Create model training scripts. Commit the model training scripts to a Git repository."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A software company builds tools for customers. The company wants to use AI to increase software development productivity. Which solution will meet these requirements?",
    "explanations": [
      "Installing code recommendation software in the company's developer tools is a practical and effective way to use AI to boost developer productivity. These tools suggest code completions, fixes, or optimizations in real time, reducing development time and errors.",
      "A binary classification model is not suitable for generating nuanced code reviews and lacks the context needed for comprehensive recommendations.",
      "A code forecasting tool may help identify risks or issues but does not directly enhance day-to-day development productivity.",
      "While NLP can be used to generate code, integrating it through code recommendation tools is more user-friendly and efficient for software developers."
    ],
    "type": "single",
    "options": [
      "Use a binary classification model to generate code reviews.",
      "Install code recommendation software in the company's developer tools.",
      "Install a code forecasting tool to predict potential code issues.",
      "Use a natural language processing (NLP) tool to generate code."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A retail store wants to predict the demand for a specific product for the next few weeks by using the Amazon SageMaker DeepAR forecasting algorithm. Which type of data will meet this requirement?",
    "explanations": [
      "Time series data is the correct input type for Amazon SageMaker DeepAR, which is specifically designed for forecasting future values based on historical time-stamped data such as product demand over time.",
      "Text data is used for natural language processing tasks, not forecasting.",
      "Image data is used in computer vision applications, not in time series forecasting.",
      "Binary data typically refers to a data format, not the structured time-based data needed for forecasting models."
    ],
    "type": "single",
    "options": [
      "Text data",
      "Image data",
      "Time series data",
      "Binary data"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A large retail bank wants to develop an ML system to help the risk management team decide on loan allocations for different demographics. What must the bank do to develop an unbiased ML model?",
    "explanations": [
      "Measuring class imbalance and adapting the training process helps reduce bias in ML models. This may include techniques like re-sampling, re-weighting, or using fairness-aware algorithms to ensure underrepresented groups are treated equitably.",
      "Reducing the size of the training dataset would likely worsen model performance and increase bias due to less representative data.",
      "Relying on historical results can perpetuate existing biases rather than address them.",
      "Creating a different model for each demographic group can introduce complexity and may reinforce segregation instead of promoting fairness."
    ],
    "type": "single",
    "options": [
      "Reduce the size of the training dataset.",
      "Ensure that the ML model predictions are consistent with historical results.",
      "Create a different ML model for each demographic group.",
      "Measure class imbalance on the training dataset. Adapt the training process accordingly."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "Which prompting technique can protect against prompt injection attacks?",
    "explanations": [
      "Adversarial prompting involves deliberately testing a model with malicious or manipulated inputs to uncover vulnerabilities such as prompt injection. This technique helps identify and mitigate risks before deployment.",
      "Zero-shot prompting provides instructions without examples but does not specifically address security threats like prompt injection.",
      "Least-to-most prompting is used to teach models to solve problems by breaking them into simpler steps and is not related to prompt security.",
      "Chain-of-thought prompting encourages step-by-step reasoning but does not offer protection against prompt injection attacks."
    ],
    "type": "single",
    "options": [
      "Adversarial prompting",
      "Zero-shot prompting",
      "Least-to-most prompting",
      "Chain-of-thought prompting"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has fine-tuned a large language model (LLM) to answer questions for a help desk. The company wants to determine if the fine-tuning has enhanced the model's accuracy. Which metric should the company use for the evaluation?",
    "explanations": [
      "F1 score is the harmonic mean of precision and recall, making it a strong metric for evaluating the accuracy of a model in tasks like question answering. It considers both false positives and false negatives, which is useful for understanding performance improvements from fine-tuning.",
      "Precision measures the correctness of positive predictions but does not account for false negatives, offering an incomplete view of accuracy.",
      "Time to first token is a performance metric related to latency, not accuracy.",
      "Word error rate is used in speech recognition and is not suitable for evaluating textual accuracy in help desk question answering tasks."
    ],
    "type": "single",
    "options": [
      "Precision",
      "Time to first token",
      "F1 score",
      "Word error rate"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is using Retrieval Augmented Generation (RAG) with Amazon Bedrock and Stable Diffusion to generate product images based on text descriptions. The results are often random and lack specific details. The company wants to increase the specificity of the generated images. Which solution meets these requirements?",
    "explanations": [
      "Increasing the classifier-free guidance (CFG) scale encourages the model to follow the input prompt more closely, resulting in more specific and accurate outputs. This helps reduce randomness and align the image generation with the intended description.",
      "Increasing the number of generation steps can improve image quality but does not necessarily make the image more aligned with the prompt.",
      "The MASK_IMAGE_BLACK option is used in inpainting tasks and does not apply to improving prompt adherence in general image generation.",
      "Increasing prompt strength may be relevant in other contexts but is not a configurable parameter for Stable Diffusion in the way CFG scale is specifically designed to control prompt adherence."
    ],
    "type": "single",
    "options": [
      "Increase the number of generation steps.",
      "Use the MASK_IMAGE_BLACK mask source option.",
      "Increase the classifier-free guidance (CFG) scale.",
      "Increase the prompt strength."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company wants to implement a large language model (LLM) based chatbot to provide customer service agents with real-time contextual responses to customers' inquiries. The company will use the company's policies as the knowledge base. Which solution will meet these requirements MOST cost-effectively?",
    "explanations": [
      "Implementing Retrieval Augmented Generation (RAG) is the most cost-effective solution for integrating company-specific knowledge without modifying the model itself. RAG retrieves relevant documents at inference time and feeds them into the LLM for contextual responses, minimizing computational and operational overhead.",
      "Retraining the LLM is extremely resource-intensive and unnecessary for incorporating company policy data.",
      "Fine-tuning is less expensive than full retraining but still incurs significant costs and reduces flexibility when updates are needed.",
      "Pre-training and data augmentation are complex and expensive processes that are typically used in the initial development of a model, not for incorporating dynamic organizational content."
    ],
    "type": "single",
    "options": [
      "Retrain the LLM on the company policy data.",
      "Fine-tune the LLM on the company policy data.",
      "Implement Retrieval Augmented Generation (RAG) for in-context responses.",
      "Use pre-training and data augmentation on the company policy data."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company wants to create a new solution by using AWS Glue. The company has minimal programming experience with AWS Glue. Which AWS service can help the company use AWS Glue?",
    "explanations": [
      "Amazon Q Developer can assist users with minimal programming experience by providing code suggestions, explanations, and examples for AWS services, including AWS Glue. It helps developers interact with Glue more effectively through natural language guidance.",
      "AWS Config is used for resource compliance and configuration tracking, not for helping with AWS Glue development.",
      "Amazon Personalize is a service for building recommendation engines, unrelated to AWS Glue.",
      "Amazon Comprehend is used for natural language processing and does not support AWS Glue development."
    ],
    "type": "single",
    "options": [
      "Amazon Q Developer",
      "AWS Config",
      "Amazon Personalize",
      "Amazon Comprehend"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company is developing a mobile ML app that uses a phone's camera to diagnose and treat insect bites. The company wants to train an image classification model by using a diverse dataset of insect bite photos from different genders, ethnicities, and geographic locations around the world. Which principle of responsible AI does the company demonstrate in this scenario?",
    "explanations": [
      "The company demonstrates the principle of Fairness by ensuring that the training dataset includes diverse examples from different genders, ethnicities, and locations. This helps the model perform equally well across demographic groups and reduces the risk of biased predictions.",
      "Explainability refers to the ability to understand and interpret how a model makes decisions, which is not the focus in this scenario.",
      "Governance involves setting policies and controls around model development and deployment, not the composition of training data.",
      "Transparency refers to clearly communicating how AI systems work, including data sources and decision-making logic, which is not being emphasized in this example."
    ],
    "type": "single",
    "options": [
      "Fairness",
      "Explainability",
      "Governance",
      "Transparency"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company is developing an ML model to make loan approvals. The company must implement a solution to detect bias in the model. The company must also be able to explain the model's predictions. Which solution will meet these requirements?",
    "explanations": [
      "Amazon SageMaker Clarify provides capabilities for detecting bias in datasets and ML models, and it also supports explainability through feature attribution techniques such as SHAP. This makes it the best solution for identifying unfair treatment in loan approval models and understanding the reasons behind model predictions.",
      "Amazon SageMaker Data Wrangler is primarily used for preparing and processing data, not for bias detection or model explainability.",
      "Amazon SageMaker Model Cards are used to document model metadata and intended usage but do not perform bias detection or explainability analysis.",
      "AWS AI Service Cards provide transparency for AWS-managed AI services, not custom ML models built by a company."
    ],
    "type": "single",
    "options": [
      "Amazon SageMaker Clarify",
      "Amazon SageMaker Data Wrangler",
      "Amazon SageMaker Model Cards",
      "AWS AI Service Cards"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has developed a generative text summarization model by using Amazon Bedrock. The company will use Amazon Bedrock automatic model evaluation capabilities. Which metric should the company use to evaluate the accuracy of the model?",
    "explanations": [
      "BERTScore is a commonly used metric for evaluating the quality of generated text, including summaries. It uses pre-trained contextual embeddings to measure semantic similarity between the generated and reference summaries, making it well-suited for generative text summarization tasks.",
      "AUC score is used for evaluating classification models, particularly for binary classification, and is not suitable for summarization.",
      "F1 score is also typically used in classification tasks and does not evaluate text generation quality.",
      "Real world knowledge (RWK) score is not a standard evaluation metric for summarization and is not supported by Amazon Bedrock's automatic evaluation capabilities."
    ],
    "type": "single",
    "options": [
      "Area Under the ROC Curve (AUC) score",
      "F1 score",
      "BERTScore",
      "Real world knowledge (RWK) score"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "An AI practitioner wants to predict the classification of flowers based on petal length, petal width, sepal length, and sepal width. Which algorithm meets these requirements?",
    "explanations": [
      "K-nearest neighbors (k-NN) is a supervised learning algorithm commonly used for classification problems. It classifies data points based on the majority class of their nearest neighbors in the feature space. It is well-suited for predicting flower species based on features like petal and sepal dimensions.",
      "K-means is an unsupervised clustering algorithm and does not perform classification based on labeled data.",
      "ARIMA is a time series forecasting model and is not applicable to classification tasks involving flower measurements.",
      "Linear regression is used for predicting continuous numerical values, not categorical classes like flower types."
    ],
    "type": "single",
    "options": [
      "K-nearest neighbors (k-NN)",
      "K-mean",
      "Autoregressive Integrated Moving Average (ARIMA)",
      "Linear regression"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company is using custom models in Amazon Bedrock for a generative AI application. The company wants to use a company managed encryption key to encrypt the model artifacts that the model customization jobs create. Which AWS service meets these requirements?",
    "explanations": [
      "AWS Key Management Service (AWS KMS) enables customers to create and manage cryptographic keys to encrypt data. Amazon Bedrock supports using customer managed keys from AWS KMS to encrypt model customization artifacts, ensuring compliance with data security policies.",
      "Amazon Inspector is used for vulnerability scanning and does not manage encryption keys.",
      "Amazon Macie is used to discover and protect sensitive data but does not manage encryption for model artifacts.",
      "AWS Secrets Manager is used to store and manage access to secrets like database credentials or API keys, not for encrypting model artifacts."
    ],
    "type": "single",
    "options": [
      "AWS Key Management Service (AWS KMS)",
      "Amazon Inspector",
      "Amazon Macie",
      "AWS Secrets Manager"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to use large language models (LLMs) to produce code from natural language code comments. Which LLM feature meets these requirements?",
    "explanations": [
      "Text generation is the correct feature because it enables LLMs to produce original content—in this case, generating code based on natural language descriptions provided in comments.",
      "Text summarization condenses long text into a shorter form and is not suitable for generating new code.",
      "Text completion continues a given prompt but is more limited and not specifically optimized for converting natural language into code.",
      "Text classification categorizes text into predefined labels and does not generate executable code."
    ],
    "type": "single",
    "options": [
      "Text summarization",
      "Text generation",
      "Text completion",
      "Text classification"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is introducing a mobile app that helps users learn foreign languages. The app makes text more coherent by calling a large language model (LLM). The company collected a diverse dataset of text and supplemented the dataset with examples of more readable versions. The company wants the LLM output to resemble the provided examples. Which metric should the company use to assess whether the LLM meets these requirements?",
    "explanations": [
      "The ROUGE score is a widely used metric for evaluating text generation tasks such as summarization or rephrasing. It measures the overlap of n-grams, word sequences, and word pairs between the generated text and reference examples, making it suitable for assessing how closely the LLM output matches the more readable examples.",
      "The value of the loss function is used during training but is not directly interpretable for evaluating model output quality.",
      "Semantic robustness refers to how well a model maintains meaning under adversarial changes, not how well it matches target examples.",
      "Latency of text generation relates to system performance and responsiveness, not content quality."
    ],
    "type": "single",
    "options": [
      "Value of the loss function",
      "Semantic robustness",
      "Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score",
      "Latency of the text generation"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company notices that its foundation model (FM) generates images that are unrelated to the prompts. The company wants to modify the prompt techniques to decrease unrelated images. Which solution meets these requirements?",
    "explanations": [
      "Negative prompts explicitly tell the model what to avoid generating. This helps refine outputs by reducing elements that are not relevant or unwanted, improving alignment between the image and the intended prompt.",
      "Zero-shot prompts involve no examples and may result in generic or less accurate outputs, especially if the prompt is not well-constructed.",
      "Positive prompts alone may encourage desirable traits but do not help eliminate irrelevant content.",
      "Ambiguous prompts increase the likelihood of unrelated results, which is counterproductive in this scenario."
    ],
    "type": "single",
    "options": [
      "Use zero-shot prompts.",
      "Use negative prompts.",
      "Use positive prompts.",
      "Use ambiguous prompts."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to use a large language model (LLM) to generate concise, feature-specific descriptions for the company’s products. Which prompt engineering technique meets these requirements?",
    "explanations": [
      "Providing detailed, product-specific prompts ensures the LLM generates accurate, concise, and tailored descriptions for each product, directly meeting the company’s requirement for precision and relevance.",
      "Creating one general prompt for all products and editing responses adds manual work and decreases prompt efficiency.",
      "Prompts by product category are more specific than general prompts but may still miss individual product nuances.",
      "Including a diverse range of features in each prompt may confuse the model or produce generic, less focused descriptions."
    ],
    "type": "single",
    "options": [
      "Create one prompt that covers all products. Edit the responses to make the responses more specific, concise, and tailored to each product.",
      "Create prompts for each product category that highlight the key features. Include the desired output format and length for each prompt response.",
      "Include a diverse range of product features in each prompt to generate creative and unique descriptions.",
      "Provide detailed, product-specific prompts to ensure precise and customized descriptions."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company is developing an ML model to predict customer churn. The model performs well on the training dataset but does not accurately predict churn for new data. Which solution will resolve this issue?",
    "explanations": [
      "The model performing well on training data but poorly on new (unseen) data is a sign of overfitting. Increasing the regularization parameter helps reduce model complexity, encouraging the model to generalize better to new data and reduce overfitting.",
      "Decreasing the regularization parameter increases model complexity and can make overfitting worse.",
      "Adding more features may introduce noise or irrelevant data, potentially increasing overfitting if not done carefully.",
      "Training the model for more epochs may exacerbate overfitting rather than resolve it."
    ],
    "type": "single",
    "options": [
      "Decrease the regularization parameter to increase model complexity.",
      "Increase the regularization parameter to decrease model complexity.",
      "Add more features to the input data.",
      "Train the model for more epochs."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "Storage and queries of embeddings from a generative AI model as vectors in the database. Which AWS service will meet these requirements?",
    "explanations": [
      "Amazon Aurora PostgreSQL supports the pgvector extension, which allows for efficient storage and querying of vector embeddings generated by generative AI models. This makes it ideal for similarity search and other vector-based operations.",
      "Amazon Athena is a query service for data stored in Amazon S3 and does not natively support vector operations.",
      "Amazon Redshift is a data warehouse optimized for analytical queries, but it does not natively support vector embeddings or similarity search.",
      "Amazon EMR is used for big data processing with frameworks like Spark and Hadoop. It can handle vectors but requires more setup and is not purpose-built for embedding queries."
    ],
    "type": "single",
    "options": [
      "Amazon Athena",
      "Amazon Aurora PostgreSQL",
      "Amazon Redshift",
      "Amazon EMR"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A financial institution is building an AI solution to make loan approval decisions by using a foundation model (FM). For security and audit purposes, the company needs the AI solution's decisions to be explainable. Which factor relates to the explainability of the AI solution's decisions?",
    "explanations": [
      "Model complexity directly impacts explainability. Simpler models like decision trees or linear models are easier to interpret and explain, whereas complex models like deep neural networks or large foundation models can be more accurate but less transparent, making their decisions harder to understand.",
      "Training time reflects how long it takes to train a model but does not determine how explainable the model is.",
      "The number of hyperparameters affects model tuning and performance, not its interpretability.",
      "Deployment time is related to operational readiness and performance, not decision explainability."
    ],
    "type": "single",
    "options": [
      "Model complexity",
      "Training time",
      "Number of hyperparameters",
      "Deployment time"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A pharmaceutical company wants to analyze user reviews of new medications and provide a concise overview for each medication. Which solution meets these requirements?",
    "explanations": [
      "Amazon Bedrock provides access to foundation models (LLMs) capable of performing natural language processing tasks such as summarization. This makes it well-suited for generating concise summaries from user reviews of medications.",
      "Amazon Personalize is used for building recommendation systems and is not designed for summarization or review analysis.",
      "Amazon SageMaker can build custom classification models, but classification is not the same as summarizing user reviews.",
      "Amazon Rekognition is used for image and video analysis, not for processing and summarizing text data."
    ],
    "type": "single",
    "options": [
      "Create a time-series forecasting model to analyze the medication reviews by using Amazon Personalize.",
      "Create medication review summaries by using Amazon Bedrock large language models (LLMs).",
      "Create a classification model that categorizes medications into different groups by using Amazon SageMaker.",
      "Create medication review summaries by using Amazon Rekognition."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company wants to build a lead prioritization application for its employees to contact potential customers. The application must give employees the ability to view and adjust the weights assigned to different variables in the model based on domain knowledge and expertise. Which ML model type meets these requirements?",
    "explanations": [
      "A logistic regression model is interpretable and allows users to view and manually adjust the weights (coefficients) associated with each input feature, making it ideal for lead prioritization scenarios where domain experts may want to influence the decision-making process.",
      "A deep learning model built on principal components lacks transparency and interpretability, and its weights are not easily accessible or modifiable.",
      "K-nearest neighbors (k-NN) is a non-parametric model that does not use explicit weights for input variables, making manual adjustment of variable importance infeasible.",
      "Neural networks involve complex interactions across many layers and parameters, making them difficult to interpret and adjust manually."
    ],
    "type": "single",
    "options": [
      "Logistic regression model",
      "Deep learning model built on principal components",
      "K-nearest neighbors (k-NN) model",
      "Neural network"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which strategy will determine if a foundation model (FM) effectively meets business objectives?",
    "explanations": [
      "Assessing the model's alignment with specific use cases is the best strategy to determine whether a foundation model effectively meets business objectives. This ensures the model performs well in the context of the company's goals and delivers tangible value.",
      "Benchmark datasets are useful for general performance comparison but may not reflect real-world business requirements.",
      "Analyzing architecture and hyperparameters is part of model development and optimization, but it does not directly connect to business value.",
      "Measuring computational resources is important for cost and scalability but does not indicate whether business objectives are being met."
    ],
    "type": "single",
    "options": [
      "Evaluate the model's performance on benchmark datasets.",
      "Analyze the model's architecture and hyperparameters.",
      "Assess the model's alignment with specific use cases.",
      "Measure the computational resources required for model deployment."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company needs to train an ML model to classify images of different types of animals. The company has a large dataset of labeled images and will not label more data. Which type of learning should the company use to train the model?",
    "explanations": [
      "Supervised learning is the appropriate approach when a labeled dataset is available. In this case, the company has labeled images of animals and wants to classify them, making supervised learning the best fit.",
      "Unsupervised learning is used when the data is not labeled and the goal is to find patterns or clusters.",
      "Reinforcement learning involves learning through rewards and penalties from interactions with an environment, which is not applicable to this classification task.",
      "Active learning involves iteratively selecting data to label to improve model performance, but the company has stated that it will not label more data."
    ],
    "type": "single",
    "options": [
      "Supervised learning",
      "Unsupervised learning",
      "Reinforcement learning",
      "Active learning"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which phase of the ML lifecycle determines compliance and regulatory requirements?",
    "explanations": [
      "Business goal identification is the phase where the organization defines the purpose of the ML solution, including aligning with compliance, legal, and regulatory requirements. This ensures that all downstream activities—such as data handling, model training, and deployment—adhere to those standards.",
      "Feature engineering focuses on preparing and transforming data for modeling and does not address regulatory concerns directly.",
      "Model training involves learning patterns from data but typically assumes that compliance requirements have already been established.",
      "Data collection implements policies but does not determine compliance needs; those are defined during business goal identification."
    ],
    "type": "single",
    "options": [
      "Feature engineering",
      "Model training",
      "Data collection",
      "Business goal identification"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A food service company wants to develop an ML model to help decrease daily food waste and increase sales revenue. The company needs to continuously improve the model's accuracy. Which solution meets these requirements?",
    "explanations": [
      "Amazon SageMaker is a fully managed service that enables the development, training, and deployment of ML models. Iterating with newer data helps continuously improve the model's accuracy in predicting demand, reducing waste, and boosting revenue.",
      "Amazon Personalize is tailored for building recommendation systems and may not directly support custom demand forecasting or waste reduction use cases.",
      "Amazon CloudWatch is used for monitoring and logging but is not an ML development tool.",
      "Amazon Rekognition is designed for image and video analysis and is not relevant to optimizing food waste or sales prediction models."
    ],
    "type": "single",
    "options": [
      "Use Amazon SageMaker and iterate with newer data.",
      "Use Amazon Personalize and iterate with historical data.",
      "Use Amazon CloudWatch to analyze customer orders.",
      "Use Amazon Rekognition to optimize the model."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company has developed an ML model to predict real estate sale prices. The company wants to deploy the model to make predictions without managing servers or infrastructure. Which solution meets these requirements?",
    "explanations": [
      "Deploying the model using an Amazon SageMaker endpoint allows the company to serve real-time predictions without managing the underlying infrastructure. SageMaker abstracts away the server management, scaling, and deployment details.",
      "Amazon EC2 requires manual provisioning and management of instances, which contradicts the goal of not managing infrastructure.",
      "Amazon EKS is a container orchestration service that requires infrastructure management and is more complex to set up for this use case.",
      "Amazon CloudFront with Amazon S3 is used for content delivery and static hosting, not for serving ML model predictions."
    ],
    "type": "single",
    "options": [
      "Deploy the model on an Amazon EC2 instance.",
      "Deploy the model on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.",
      "Deploy the model by using Amazon CloudFront with an Amazon S3 integration.",
      "Deploy the model by using an Amazon SageMaker endpoint."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to develop an AI application to help its employees check open customer claims, identify details for a specific claim, and access documents for a claim. Which solution meets these requirements?",
    "explanations": [
      "Agents for Amazon Bedrock can orchestrate multi-step tasks and call APIs or retrieve information from connected knowledge bases, such as claim documents. When combined with Amazon Bedrock knowledge bases, the solution can provide context-aware, real-time access to customer claim information.",
      "Amazon Fraud Detector is specialized for detecting fraud, not for handling customer claim workflows.",
      "Amazon Personalize is designed for recommendations, not task orchestration or document retrieval.",
      "Amazon SageMaker is powerful for custom ML models but would require significantly more development and infrastructure effort compared to Bedrock agents and knowledge bases for this use case."
    ],
    "type": "single",
    "options": [
      "Use Agents for Amazon Bedrock with Amazon Fraud Detector to build the application.",
      "Use Agents for Amazon Bedrock with Amazon Bedrock knowledge bases to build the application.",
      "Use Amazon Personalize with Amazon Bedrock knowledge bases to build the application.",
      "Use Amazon SageMaker to build the application by training a new ML model."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A manufacturing company uses AI to inspect products and find any damages or defects. Which type of AI application is the company using?",
    "explanations": [
      "Computer vision is the field of AI that enables machines to interpret and analyze visual data from the world, such as images or video. In this case, the AI is being used to inspect products for damages or defects, which is a classic use case for computer vision.",
      "Recommendation systems are used to suggest products or content to users based on preferences, not for visual inspection.",
      "Natural language processing (NLP) focuses on understanding and generating human language, not analyzing visual content.",
      "Image processing is a lower-level technique often used in computer vision, but it does not encompass the broader AI-driven interpretation and decision-making needed for defect detection."
    ],
    "type": "single",
    "options": [
      "Recommendation system",
      "Natural language processing (NLP)",
      "Computer vision",
      "Image processing"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company wants to create an ML model to predict customer satisfaction. The company needs fully automated model tuning. Which AWS service meets these requirements?",
    "explanations": [
      "Amazon SageMaker provides built-in support for automated model tuning, also known as hyperparameter optimization. It automatically finds the best combination of model parameters to improve accuracy and performance for predictive tasks like customer satisfaction prediction.",
      "Amazon Personalize is focused on real-time personalized recommendations and is not designed for general-purpose model tuning.",
      "Amazon Athena is an interactive query service for analyzing data in Amazon S3 using SQL and is not used for model development or tuning.",
      "Amazon Comprehend is a natural language processing service that helps analyze text but does not offer automated model tuning capabilities."
    ],
    "type": "single",
    "options": [
      "Amazon Personalize",
      "Amazon SageMaker",
      "Amazon Athena",
      "Amazon Comprehend"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "Which technique can a company use to lower bias and toxicity in generative AI applications during the post-processing ML lifecycle?",
    "explanations": [
      "Human-in-the-loop is a post-processing technique where human reviewers evaluate and filter AI outputs to ensure they meet safety, fairness, and quality standards. It is particularly effective in reducing bias and toxicity in generative AI outputs before final delivery.",
      "Data augmentation is a technique used during training to improve model generalization and does not directly address post-processing bias.",
      "Feature engineering involves modifying or creating features before training and is not part of the post-processing stage.",
      "Adversarial training is used to improve model robustness against adversarial attacks and is typically part of training, not post-processing."
    ],
    "type": "single",
    "options": [
      "Human-in-the-loop",
      "Data augmentation",
      "Feature engineering",
      "Adversarial training"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A bank has fine-tuned a large language model (LLM) to expedite the loan approval process. During an external audit of the model, the company discovered that the model was approving loans at a faster pace for a specific demographic than for other demographics. How should the bank fix this issue MOST cost-effectively?",
    "explanations": [
      "Including more diverse training data and re-fine-tuning the model helps address the bias in a targeted and cost-effective way, without the expense of pre-training a new model from scratch. It allows the existing fine-tuned model to better generalize across demographics.",
      "RAG is used to enhance factual accuracy and context in model outputs but does not address fairness or bias in decision-making.",
      "AWS Trusted Advisor is used for best practice checks in security and cost optimization, not for ML bias detection or mitigation.",
      "Pre-training a new LLM is highly resource-intensive and expensive, making it the least cost-effective option in this context."
    ],
    "type": "single",
    "options": [
      "Include more diverse training data. Fine-tune the model again by using the new data.",
      "Use Retrieval Augmented Generation (RAG) with the fine-tuned model.",
      "Use AWS Trusted Advisor checks to eliminate bias.",
      "Pre-train a new LLM with more diverse training data."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company needs to log all requests made to its Amazon Bedrock API. The company must retain the logs securely for 5 years at the lowest possible cost. Which combination of AWS service and storage class meets these requirements? (Choose two.)",
    "explanations": [
      "AWS CloudTrail captures and logs API calls made to Amazon Bedrock and other AWS services, fulfilling the requirement to log all requests.",
      "Amazon S3 Intelligent-Tiering automatically moves data between storage tiers based on access patterns, providing cost-effective long-term storage while meeting retention requirements.",
      "Amazon CloudWatch is primarily used for monitoring and real-time log analysis, but it's more expensive for long-term retention.",
      "AWS Audit Manager helps with compliance frameworks but does not directly capture API logs.",
      "Amazon S3 Standard is costlier than Intelligent-Tiering for long-term storage, especially for infrequently accessed data."
    ],
    "type": "multiple",
    "options": [
      "AWS CloudTrail",
      "Amazon CloudWatch",
      "AWS Audit Manager",
      "Amazon S3 Intelligent-Tiering",
      "Amazon S3 Standard"
    ],
    "correctAnswer": [
      "A",
      "D"
    ]
  },
  {
    "text": "An ecommerce company wants to improve search engine recommendations by customizing the results for each user of the company’s ecommerce platform. Which AWS service meets these requirements?",
    "explanations": [
      "Amazon Personalize is designed to deliver real-time personalized recommendations based on user behavior and preferences, making it ideal for customizing search engine results in an ecommerce platform.",
      "Amazon Kendra is an intelligent search service used for enterprise document search but not for personalized ecommerce recommendations.",
      "Amazon Rekognition is used for image and video analysis, not search or recommendations.",
      "Amazon Transcribe converts speech to text and is unrelated to search personalization."
    ],
    "type": "single",
    "options": [
      "Amazon Personalize",
      "Amazon Kendra",
      "Amazon Rekognition",
      "Amazon Transcribe"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A hospital is developing an AI system to assist doctors in diagnosing diseases based on patient records and medical images. To comply with regulations, the sensitive patient data must not leave the country the data is located in. Which data governance strategy will ensure compliance and protect patient privacy?",
    "explanations": [
      "Data residency refers to the requirement that data must be stored and processed within a specific geographic region or country. This is essential for complying with data protection regulations like HIPAA, GDPR, or country-specific health data laws, which often mandate that sensitive data, such as patient records, remain within national borders.",
      "Data quality focuses on the accuracy, completeness, and reliability of data, not its geographic storage.",
      "Data discoverability refers to how easily data can be located and accessed, which does not address compliance with geographic restrictions.",
      "Data enrichment involves enhancing data with additional context or attributes, but it does not relate to geographic or regulatory constraints."
    ],
    "type": "single",
    "options": [
      "Data residency",
      "Data quality",
      "Data discoverability",
      "Data enrichment"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company needs to monitor the performance of its ML systems by using a highly scalable AWS service. Which AWS service meets these requirements?",
    "explanations": [
      "Amazon CloudWatch is a highly scalable monitoring and observability service that collects metrics, logs, and events. It is designed to monitor the performance and health of AWS resources and applications, including ML systems.",
      "AWS CloudTrail captures API call logs for auditing and security purposes, not real-time performance monitoring.",
      "AWS Trusted Advisor provides best practice recommendations but does not offer real-time monitoring of system performance.",
      "AWS Config tracks configuration changes and compliance over time but is not designed for performance monitoring."
    ],
    "type": "single",
    "options": [
      "Amazon CloudWatch",
      "AWS CloudTrail",
      "AWS Trusted Advisor",
      "AWS Config"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "An AI practitioner is developing a prompt for an Amazon Titan model. The model is hosted on Amazon Bedrock. The AI practitioner is using the model to solve numerical reasoning challenges. The AI practitioner adds the following phrase to the end of the prompt: “Ask the model to show its work by explaining its reasoning step by step.” Which prompt engineering technique is the AI practitioner using?",
    "explanations": [
      "Chain-of-thought prompting encourages the model to break down complex reasoning problems by explaining each step of the thought process. This improves the model’s ability to handle numerical and logical reasoning tasks.",
      "Prompt injection refers to a security vulnerability where the model is tricked into ignoring original instructions, not a deliberate reasoning strategy.",
      "Few-shot prompting involves giving the model a few examples to learn from, which is not the case here.",
      "Prompt templating involves creating reusable prompt formats, which is not directly reflected in the given prompt."
    ],
    "type": "single",
    "options": [
      "Chain-of-thought prompting",
      "Prompt injection",
      "Few-shot prompting",
      "Prompt templating"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which AWS service makes foundation models (FMs) available to help users build and scale generative AI applications?",
    "explanations": [
      "Amazon Bedrock is the AWS service that provides access to a variety of foundation models (FMs) from leading model providers, allowing users to build and scale generative AI applications without managing infrastructure.",
      "Amazon Q Developer is a generative AI-powered assistant for software development tasks, not a platform for accessing multiple FMs.",
      "Amazon Kendra is an enterprise search service, not a generative AI platform.",
      "Amazon Comprehend is a natural language processing (NLP) service for extracting insights from text, but it does not provide access to foundation models."
    ],
    "type": "single",
    "options": [
      "Amazon Q Developer",
      "Amazon Bedrock",
      "Amazon Kendra",
      "Amazon Comprehend"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company is building a mobile app for users who have a visual impairment. The app must be able to hear what users say and provide voice responses. Which solution will meet these requirements?",
    "explanations": [
      "Using a deep learning neural network to perform speech recognition allows the app to understand spoken language, which is essential for interacting with users who have visual impairments. Coupled with text-to-speech for voice responses, this meets both input and output requirements.",
      "Searching for patterns in numeric data is unrelated to speech recognition or audio interaction.",
      "Generative AI summarization creates condensed text, which is not suitable for real-time voice interaction.",
      "Image classification and recognition are used for visual inputs, which do not address the needs of visually impaired users."
    ],
    "type": "single",
    "options": [
      "Use a deep learning neural network to perform speech recognition.",
      "Build ML models to search for patterns in numeric data.",
      "Use generative AI summarization to generate human-like text.",
      "Build custom models for image classification and recognition."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company wants to enhance response quality for a large language model (LLM) for complex problem-solving tasks. The tasks require detailed reasoning and a step-by-step explanation process. Which prompt engineering technique meets these requirements?",
    "explanations": [
      "Chain-of-thought prompting is designed to guide large language models to solve complex problems by explicitly prompting them to explain their reasoning step-by-step. This improves accuracy and interpretability in tasks requiring logical and multi-step thinking.",
      "Few-shot prompting provides a few examples to help the model generalize, but it does not explicitly guide reasoning in step-by-step form.",
      "Zero-shot prompting provides only task instructions without examples or reasoning guidance, which may not be effective for complex tasks.",
      "Directional stimulus prompting is a less common technique and not specifically designed for structured reasoning or problem-solving tasks."
    ],
    "type": "single",
    "options": [
      "Few-shot prompting",
      "Zero-shot prompting",
      "Directional stimulus prompting",
      "Chain-of-thought prompting"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to keep its foundation model (FM) relevant by using the most recent data. The company wants to implement a model training strategy that includes regular updates to the FM. Which solution meets these requirements?",
    "explanations": [
      "Continuous pre-training is a strategy where a foundation model is periodically updated with new data to remain current and aligned with recent information. This is ideal for keeping models relevant over time.",
      "Batch learning trains models in discrete batches and is not typically used for regular updates with new data.",
      "Static training involves training a model once on a fixed dataset, without incorporating updates.",
      "Latent training is not a standard or widely recognized training methodology in machine learning."
    ],
    "type": "single",
    "options": [
      "Batch learning",
      "Continuous pre-training",
      "Static training",
      "Latent training"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "Which option is a characteristic of AI governance frameworks for building trust and deploying human-centered AI technologies?",
    "explanations": [
      "Developing policies and guidelines for data, transparency, responsible AI, and compliance is a core characteristic of AI governance frameworks. These frameworks are designed to ensure ethical, safe, and trustworthy deployment of AI technologies that align with societal and regulatory expectations.",
      "Expanding initiatives across business units and creating business value are outcomes of successful AI strategy, not specific characteristics of governance frameworks.",
      "Aligning with revenue goals and stakeholder expectations is important but falls under business strategy rather than governance.",
      "Overcoming challenges for transformation and growth refers to organizational execution and change management, not the framework for AI governance."
    ],
    "type": "single",
    "options": [
      "Expanding initiatives across business units to create long-term business value",
      "Ensuring alignment with business standards, revenue goals, and stakeholder expectations",
      "Overcoming challenges to drive business transformation and growth",
      "Developing policies and guidelines for data, transparency, responsible AI, and compliance"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "An ecommerce company is using a generative AI chatbot to respond to customer inquiries. The company wants to measure the financial effect of the chatbot on the company’s operations. Which metric should the company use?",
    "explanations": [
      "Cost for each customer conversation directly reflects the financial impact of using the generative AI chatbot. It helps the company understand how much they are spending per interaction and compare that to the cost of human agent interactions or other support channels.",
      "Number of customer inquiries handled is a volume metric, not a financial one.",
      "Cost of training AI models is a one-time or periodic expense and does not reflect ongoing operational costs.",
      "Average handled time (AHT) measures efficiency but doesn’t directly indicate financial effect unless paired with cost data."
    ],
    "type": "single",
    "options": [
      "Number of customer inquiries handled",
      "Cost of training AI models",
      "Cost for each customer conversation",
      "Average handled time (AHT)"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company wants to find groups for its customers based on the customers’ demographics and buying patterns. Which algorithm should the company use to meet this requirement?",
    "explanations": [
      "K-means is an unsupervised learning algorithm used for clustering. It is ideal for grouping customers based on similar characteristics such as demographics and buying behavior without the need for labeled outcomes.",
      "K-nearest neighbors (k-NN) is a supervised learning algorithm used for classification and regression, not for clustering.",
      "Decision trees are used for classification and regression tasks, requiring labeled data.",
      "Support vector machines (SVMs) are also supervised and best suited for classification and regression problems, not for finding natural groupings in data."
    ],
    "type": "single",
    "options": [
      "K-nearest neighbors (k-NN)",
      "K-means",
      "Decision tree",
      "Support vector machine"
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company’s large language model (LLM) is experiencing hallucinations. How can the company decrease hallucinations?",
    "explanations": [
      "Decreasing the temperature inference parameter makes the model's outputs more deterministic and less random, which can help reduce hallucinations—unfounded or factually incorrect responses from the model.",
      "Agents for Amazon Bedrock are used for orchestrating workflows and executing tasks, not for supervising model training or reducing hallucinations.",
      "While data pre-processing is important, hallucinations typically stem from the model's generative behavior rather than specific data artifacts, especially in pre-trained models.",
      "No foundation model can be entirely 'trained to not hallucinate.' All generative models have the potential to hallucinate, though some may be better tuned than others. Reducing temperature is a more direct and cost-effective approach."
    ],
    "type": "single",
    "options": [
      "Set up Agents for Amazon Bedrock to supervise the model training.",
      "Use data pre-processing and remove any data that causes hallucinations.",
      "Decrease the temperature inference parameter for the model.",
      "Use a foundation model (FM) that is trained to not hallucinate."
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company is using a large language model (LLM) on Amazon Bedrock to build a chatbot. The chatbot processes customer support requests. To resolve a request, the customer and the chatbot must interact a few times. Which solution gives the LLM the ability to use content from previous customer messages?",
    "explanations": [
      "Adding previous messages to the model prompt is the correct way to provide conversational context to a stateless LLM, enabling it to generate coherent and context-aware responses across multiple turns.",
      "Turning on model invocation logging helps with auditing and debugging but does not influence the model’s behavior in the current prompt.",
      "Amazon Personalize is designed for personalized recommendations, not for managing or injecting conversational history into prompts.",
      "Provisioned Throughput improves performance and scalability but does not affect context awareness or memory across turns."
    ],
    "type": "single",
    "options": [
      "Turn on model invocation logging to collect messages.",
      "Add messages to the model prompt.",
      "Use Amazon Personalize to save conversation history.",
      "Use Provisioned Throughput for the LLM."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A company’s employees provide product descriptions and recommendations to customers when customers call the customer service center. These recommendations are based on where the customers are located. The company wants to use foundation models (FMs) to automate this process. Which AWS service meets these requirements?",
    "explanations": [
      "Amazon Bedrock provides access to foundation models (FMs) that can be used to generate personalized product descriptions and recommendations based on customer location and other context. It enables building generative AI applications without managing infrastructure.",
      "Amazon Macie is a security service that detects sensitive data and does not support generating recommendations or descriptions.",
      "Amazon Transcribe converts speech to text but does not generate content or automate recommendations.",
      "Amazon Textract extracts text from documents and is not used for generating personalized responses or handling conversational AI tasks."
    ],
    "type": "single",
    "options": [
      "Amazon Macie",
      "Amazon Transcribe",
      "Amazon Bedrock",
      "Amazon Textract"
    ],
    "correctAnswer": [
      "C"
    ]
  },
  {
    "text": "A company wants to upload customer service email messages to Amazon S3 to develop a business analysis application. The messages sometimes contain sensitive data. The company wants to receive an alert every time sensitive information is found. Which solution fully automates the sensitive information detection process with the LEAST development effort?",
    "explanations": [
      "Amazon Macie is a fully managed data security and privacy service that uses machine learning to automatically detect and alert on sensitive data in S3. It requires minimal setup and offers low operational overhead, making it the most efficient solution for this use case.",
      "Deploying an LLM via Amazon SageMaker requires significant development and infrastructure setup, making it more complex.",
      "Manually developing regex patterns is labor-intensive, error-prone, and not scalable for comprehensive data protection.",
      "Asking customers to avoid sharing sensitive information is unreliable and does not meet compliance or automation requirements."
    ],
    "type": "single",
    "options": [
      "Configure Amazon Macie to detect sensitive information in the documents that are uploaded to Amazon S3.",
      "Use Amazon SageMaker endpoints to deploy a large language model (LLM) to redact sensitive data.",
      "Develop multiple regex patterns to detect sensitive data. Expose the regex patterns on an Amazon SageMaker notebook.",
      "Ask the customers to avoid sharing sensitive information in their email messages."
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "Which option is a benefit of using Amazon SageMaker Model Cards to document AI models?",
    "explanations": [
      "Amazon SageMaker Model Cards provide a standardized way to document important details about an AI model, including its purpose, training data, evaluation metrics, intended use cases, and limitations. This helps promote transparency, governance, and responsible AI practices.",
      "While model cards can be visually organized, their primary benefit is not visual appeal but standardization and documentation.",
      "Model cards do not reduce computational requirements; they serve as documentation tools.",
      "Model cards are not used for physically storing models—they document the metadata and details about models, not the models themselves."
    ],
    "type": "single",
    "options": [
      "Providing a visually appealing summary of a model’s capabilities.",
      "Standardizing information about a model’s purpose, performance, and limitations.",
      "Reducing the overall computational requirements of a model.",
      "Physically storing models for archival purposes."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "What does an F1 score measure in the context of foundation model (FM) performance?",
    "explanations": [
      "The F1 score is a metric that combines precision and recall into a single number by calculating their harmonic mean. It is especially useful in scenarios where there is an imbalance between classes or when both false positives and false negatives are important to consider.",
      "Model speed in generating responses is typically measured in terms like latency or time to first token, not F1 score.",
      "Financial cost is unrelated to the F1 score, which measures prediction accuracy.",
      "Energy efficiency involves metrics like FLOPs or power consumption, not the F1 score."
    ],
    "type": "single",
    "options": [
      "Model precision and recall",
      "Model speed in generating responses",
      "Financial cost of operating the model",
      "Energy efficiency of the model’s computations"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A company deployed an AI/ML solution to help customer service agents respond to frequently asked questions. The questions can change over time. The company wants to give customer service agents the ability to ask questions and receive automatically generated answers to common customer questions. Which strategy will meet these requirements MOST cost-effectively?",
    "explanations": [
      "Using Retrieval Augmented Generation (RAG) with prompt engineering is the most cost-effective approach. RAG retrieves relevant documents or knowledge base content at inference time, allowing the model to generate accurate, context-aware responses without requiring expensive retraining or fine-tuning.",
      "Regular fine-tuning is resource-intensive and not efficient for frequently changing FAQs.",
      "Training a model using context data implies building a custom model, which is costly and time-consuming.",
      "Pre-training is even more expensive and typically done once by foundational model providers, not by end-users for use-case updates."
    ],
    "type": "single",
    "options": [
      "Fine-tune the model regularly.",
      "Train the model by using context data.",
      "Pre-train and benchmark the model by using context data.",
      "Use Retrieval Augmented Generation (RAG) with prompt engineering techniques."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company built an AI-powered resume screening system. The company used a large dataset to train the model. The dataset contained resumes that were not representative of all demographics. Which core dimension of responsible AI does this scenario present?",
    "explanations": [
      "This scenario highlights the issue of **fairness**, as the training data is not representative of all demographics. This can lead to biased decisions by the AI system, favoring certain groups over others, which is a key concern in responsible AI development.",
      "Explainability refers to how well the decisions of the AI can be understood, but it is not the primary issue here.",
      "Privacy and security focus on protecting sensitive data, which is not the main concern in this example.",
      "Transparency involves being open about how the model works and what data it uses, but the core issue in this case is biased data leading to unfair outcomes."
    ],
    "type": "single",
    "options": [
      "Fairness",
      "Explainability",
      "Privacy and security",
      "Transparency"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A global financial company has developed an ML application to analyze stock market data and provide stock market trends. The company wants to continuously monitor the application development phases and to ensure that company policies and industry regulations are followed. Which AWS services will help the company assess compliance requirements? (Choose two.)",
    "explanations": [
      "AWS Audit Manager helps continuously assess and audit AWS resource usage to determine compliance with company policies and industry regulations. It automates the collection of evidence for audits.",
      "AWS Config enables monitoring and recording of resource configurations and changes to assess compliance with internal policies and external regulations.",
      "Amazon Inspector is used for security assessments and vulnerability management but does not focus on regulatory compliance directly.",
      "Amazon CloudWatch is a monitoring service for performance metrics and logs, not compliance tracking.",
      "AWS CloudTrail logs API calls and activity but does not provide automated compliance assessments like AWS Audit Manager or AWS Config."
    ],
    "type": "multiple",
    "options": [
      "AWS Audit Manager",
      "AWS Config",
      "Amazon Inspector",
      "Amazon CloudWatch",
      "AWS CloudTrail"
    ],
    "correctAnswer": [
      "A",
      "B"
    ]
  },
  {
    "text": "A company wants to improve the accuracy of the responses from a generative AI application. The application uses a foundation model (FM) on Amazon Bedrock. Which solution meets these requirements MOST cost-effectively?",
    "explanations": [
      "Using prompt engineering is the most cost-effective solution to improve the accuracy of responses. By carefully crafting the input prompts, the company can guide the FM to produce more relevant and accurate outputs without the need for expensive model training or fine-tuning.",
      "Fine-tuning an FM can improve accuracy but involves significant cost and complexity compared to prompt engineering.",
      "Retraining an FM requires large-scale data and compute resources, making it highly expensive and impractical for most use cases.",
      "Training a new FM from scratch is extremely resource-intensive and unnecessary when accuracy can often be improved through better prompting techniques."
    ],
    "type": "single",
    "options": [
      "Fine-tune the FM.",
      "Retrain the FM.",
      "Train a new FM.",
      "Use prompt engineering."
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company wants to identify harmful language in the comments section of social media posts by using an ML model. The company will not use labeled data to train the model. Which strategy should the company use to identify harmful language?",
    "explanations": [
      "Amazon Comprehend toxicity detection allows companies to detect harmful or offensive language in text without requiring labeled training data. It is a pre-trained NLP service designed for tasks like sentiment analysis and content moderation.",
      "Amazon Rekognition is for analyzing images and videos, not text comments.",
      "Amazon SageMaker built-in algorithms require labeled data for supervised learning, which the company does not plan to use.",
      "Amazon Polly converts text to speech and is unrelated to detecting harmful language."
    ],
    "type": "single",
    "options": [
      "Use Amazon Rekognition moderation.",
      "Use Amazon Comprehend toxicity detection.",
      "Use Amazon SageMaker built-in algorithms to train the model.",
      "Use Amazon Polly to monitor comments."
    ],
    "correctAnswer": [
      "B"
    ]
  },
  {
    "text": "A media company wants to analyze viewer behavior and demographics to recommend personalized content. The company wants to deploy a customized ML model in its production environment. The company also wants to observe if the model quality drifts over time.  Which AWS service or feature meets these requirements?",
    "explanations": [
      "Amazon SageMaker Model Monitor continuously monitors machine learning models in production to detect data and model quality drift. It helps ensure that the model maintains its performance over time.",
      "Amazon Rekognition is used for image and video analysis, not for monitoring model performance.",
      "Amazon SageMaker Clarify helps detect bias and provides explainability for ML models but does not monitor model quality drift.",
      "Amazon Comprehend is a natural language processing service, not intended for monitoring ML model drift."
    ],
    "type": "single",
    "options": [
      "Amazon Rekognition",
      "Amazon SageMaker Clarify",
      "Amazon Comprehend",
      "Amazon SageMaker Model Monitor"
    ],
    "correctAnswer": [
      "D"
    ]
  },
  {
    "text": "A company is deploying AI/ML models by using AWS services. The company wants to offer transparency into the models’ decision-making processes and provide explanations for the model outputs. Which AWS service or feature meets these requirements?",
    "explanations": [
      "Amazon SageMaker Model Cards provide a standardized way to document ML models, including details on model purpose, performance, intended use, limitations, and ethical considerations. This promotes transparency and helps stakeholders understand the model’s decision-making process.",
      "Amazon Rekognition is used for analyzing images and videos, and does not provide model explainability or transparency features.",
      "Amazon Comprehend is used for natural language processing tasks but does not offer tools for explaining model decisions.",
      "Amazon Lex is a service for building conversational interfaces and chatbots and does not address model documentation or transparency."
    ],
    "type": "single",
    "options": [
      "Amazon SageMaker Model Cards",
      "Amazon Rekognition",
      "Amazon Comprehend",
      "Amazon Lex"
    ],
    "correctAnswer": [
      "A"
    ]
  },
  {
    "text": "A manufacturing company wants to create product descriptions in multiple languages. Which AWS service will automate this task?",
    "explanations": [
      "Amazon Translate is a neural machine translation service that automatically translates text between languages, making it the best choice for creating product descriptions in multiple languages.",
      "Amazon Transcribe converts speech to text and is not used for translating content.",
      "Amazon Kendra is an enterprise search service and does not perform language translation.",
      "Amazon Polly converts text to lifelike speech but does not translate text between languages."
    ],
    "type": "single",
    "options": [
      "Amazon Translate",
      "Amazon Transcribe",
      "Amazon Kendra",
      "Amazon Polly"
    ],
    "correctAnswer": [
      "A"
    ]
  }
]